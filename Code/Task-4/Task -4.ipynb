{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184a76cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b854d37",
   "metadata": {},
   "source": [
    "For each of the target action, created a json as part of task-5\n",
    "Also, replace the input_folder locations to the location of the target videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643df9d",
   "metadata": {},
   "source": [
    "# Taget Video - 'Sword'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f99803",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/srirupin/Downloads/target/sword'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74af5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec079b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srirupin/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/srirupin/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "# Define layers to capture\n",
    "desired_layers = ['layer3']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step\n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach  \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() \n",
    "    #Defining the maximum frames to 32 and step as 16\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "     #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "         # Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    # Stack outputs along a new dimension\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    fully_connected = nn.Linear(256, 512)\n",
    "    output_current_layer = fully_connected(output_current_layer)\n",
    "    layer3_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393c4daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/sword/RETURN_OF_THE_KING_sword_u_nm_np1_fr_bad_42.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f485d5",
   "metadata": {},
   "source": [
    "# For layer - 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f76939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5731d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the output\n",
    "desired_layers = ['layer4']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step\n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach     \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #Defining the maximum frames to 32 and step as 16\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        #Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    # Stack outputs along a new dimension\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    layer4_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a93b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/sword/RETURN_OF_THE_KING_sword_u_nm_np1_fr_bad_42.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9ec5b",
   "metadata": {},
   "source": [
    "# Task - c Avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34e0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dfa8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the output\n",
    "desired_layers = ['avgpool']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step\n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach     \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #print(len(video_frames))\n",
    "    #print(len(video_frames))\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #print(\"The slided frames are: \", len(sliding_frames))\n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        #transformed_frame=transform_frame\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #print(\"The Slide Output is: \", len(slided_output_layer))\n",
    "    # Stack outputs along a new dimension\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    avgpool_output.append(output_current_layer.tolist())\n",
    "    #print(\"The output is: \", output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00087806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/sword/RETURN_OF_THE_KING_sword_u_nm_np1_fr_bad_42.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0883422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "126\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "print(len(layer3_output))\n",
    "print(len(layer4_output))\n",
    "print(len(avgpool_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032488e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sword_output =[]\n",
    "sword_output.append(layer3_output)\n",
    "sword_output.append(layer4_output)\n",
    "sword_output.append(avgpool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d79b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"sword_output.json\", \"w\") as file:\n",
    "    json.dump(sword_output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517858e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba32e6bd",
   "metadata": {},
   "source": [
    "# Target Video - Cart Wheel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a318276",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/srirupin/Downloads/target/cartwheel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f5d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "151a53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the output\n",
    "desired_layers = ['layer3']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step\n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach\n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #print(len(video_frames))\n",
    "    #print(len(video_frames))\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #print(\"The slided frames are: \", len(sliding_frames))\n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        #transformed_frame=transform_frame\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        #Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #print(\"The Slide Output is: \", len(slided_output_layer))\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    fully_connected = nn.Linear(256, 512)\n",
    "    output_current_layer = fully_connected(output_current_layer)\n",
    "    layer3_output.append(output_current_layer.tolist())\n",
    "    #print(\"The output is: \", output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f0b4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f4e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cacbd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the output\n",
    "desired_layers = ['layer4']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach\n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #print(len(video_frames))\n",
    "    #print(len(video_frames))\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #print(\"The slided frames are: \", len(sliding_frames))\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        #transformed_frame=transform_frame\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #print(\"The Slide Output is: \", len(slided_output_layer))\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    layer4_output.append(output_current_layer.tolist())\n",
    "    #print(\"The output is: \", output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dee5df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c0dfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fb4d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the output\n",
    "desired_layers = ['avgpool']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step  \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach    \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #print(len(video_frames))\n",
    "    #print(len(video_frames))\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #print(\"The slided frames are: \", len(sliding_frames))\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        #transformed_frame=transform_frame\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #print(\"The Slide Output is: \", len(slided_output_layer))\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    avgpool_output.append(output_current_layer.tolist())\n",
    "    #print(\"The output is: \", output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45393c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb3d0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartwheel_output =[]\n",
    "cartwheel_output.append(layer3_output)\n",
    "cartwheel_output.append(layer4_output)\n",
    "cartwheel_output.append(avgpool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9ca6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#sword_output = sword_output.tolist()\n",
    "with open(\"cartwheel_output.json\", \"w\") as file:\n",
    "    json.dump(cartwheel_output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2fa94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ddf3f6e",
   "metadata": {},
   "source": [
    "# Target Video - drink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0ce46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/srirupin/Downloads/target/drink'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baa35f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75b15390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the output\n",
    "desired_layers = ['layer3']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach       \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #print(len(video_frames))\n",
    "    #print(len(video_frames))\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #print(\"The slided frames are: \", len(sliding_frames))\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        #transformed_frame=transform_frame\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #print(\"The Slide Output is: \", len(slided_output_layer))\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    fully_connected = nn.Linear(256, 512)\n",
    "    output_current_layer = fully_connected(output_current_layer)\n",
    "    layer3_output.append(output_current_layer.tolist())\n",
    "    #print(\"The output is: \", output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da3aaf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/drink/TheBoondockSaints_drink_u_nm_np1_fr_med_75.avi\n",
      "Skipping the video /Users/srirupin/Downloads/target/drink/BLACK_HAWK_DOWN_drink_h_nm_np1_fr_bad_36.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf21c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5b1282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the output\n",
    "desired_layers = ['layer4']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step   \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach       \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #print(len(video_frames))\n",
    "    #print(len(video_frames))\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #print(\"The slided frames are: \", len(sliding_frames))\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        #transformed_frame=transform_frame\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #print(\"The Slide Output is: \", len(slided_output_layer))\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    layer4_output.append(output_current_layer.tolist())\n",
    "    #print(\"The output is: \", output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ffe347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/drink/TheBoondockSaints_drink_u_nm_np1_fr_med_75.avi\n",
      "Skipping the video /Users/srirupin/Downloads/target/drink/BLACK_HAWK_DOWN_drink_h_nm_np1_fr_bad_36.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70bcd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7b6466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['avgpool']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step   \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach       \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #print(len(video_frames))\n",
    "    #print(len(video_frames))\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #print(\"The slided frames are: \", len(sliding_frames))\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        #transformed_frame=transform_frame\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #print(\"The Slide Output is: \", len(slided_output_layer))\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    avgpool_output.append(output_current_layer.tolist())\n",
    "    #print(\"The output is: \", output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c456742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/drink/TheBoondockSaints_drink_u_nm_np1_fr_med_75.avi\n",
      "Skipping the video /Users/srirupin/Downloads/target/drink/BLACK_HAWK_DOWN_drink_h_nm_np1_fr_bad_36.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0eb1a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drink_output =[]\n",
    "drink_output.append(layer3_output)\n",
    "drink_output.append(layer4_output)\n",
    "drink_output.append(avgpool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ffe2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#sword_output = sword_output.tolist()\n",
    "with open(\"drink_output.json\", \"w\") as file:\n",
    "    json.dump(drink_output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b53c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da4a84c",
   "metadata": {},
   "source": [
    "# Target Video - ride_bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12c61ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/srirupin/Downloads/target/ride_bike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fec691bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "104a1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['layer3']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step    \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach\n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #print(len(video_frames))\n",
    "    #print(len(video_frames))\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #print(\"The slided frames are: \", len(sliding_frames))\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        #transformed_frame=transform_frame\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        #Append the layers\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #print(\"The Slide Output is: \", len(slided_output_layer))\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    fully_connected = nn.Linear(256, 512)\n",
    "    output_current_layer = fully_connected(output_current_layer)\n",
    "    layer3_output.append(output_current_layer.tolist())\n",
    "    #print(\"The output is: \", output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2633fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62654f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acbc48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['layer4']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step     \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach    \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    layer4_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00e1e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b653777",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cefcd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['avgpool']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step     \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach    \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    avgpool_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32ecb50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6f78dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridebike_output =[]\n",
    "ridebike_output.append(layer3_output)\n",
    "ridebike_output.append(layer4_output)\n",
    "ridebike_output.append(avgpool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "316bea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#sword_output = sword_output.tolist()\n",
    "with open(\"ridebikeoutput.json\", \"w\") as file:\n",
    "    json.dump(ridebike_output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e1aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e064bc23",
   "metadata": {},
   "source": [
    "# Target Video - sword_exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a4b6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/srirupin/Downloads/target/sword_exercise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c13c0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57264646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['layer3']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step   \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach      \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        #Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    fully_connected = nn.Linear(256, 512)\n",
    "    output_current_layer = fully_connected(output_current_layer)\n",
    "    layer3_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57d3e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/sword_exercise/Cold_Steel_Scottish_Broad_Sword_sword_exercise_u_cm_np1_fr_bad_1.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0da3c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93d2b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['layer4']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step  \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach    \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #Transform the layers\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        #Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    layer4_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01c9fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/sword_exercise/Cold_Steel_Scottish_Broad_Sword_sword_exercise_u_cm_np1_fr_bad_1.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1305c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27d5e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['avgpool']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step    \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach        \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        #Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    avgpool_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c848037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/sword_exercise/Cold_Steel_Scottish_Broad_Sword_sword_exercise_u_cm_np1_fr_bad_1.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c6d7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "swordexcercise_output =[]\n",
    "swordexcercise_output.append(layer3_output)\n",
    "swordexcercise_output.append(layer4_output)\n",
    "swordexcercise_output.append(avgpool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d55eab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#sword_output = sword_output.tolist()\n",
    "with open(\"swordexcercise_output.json\", \"w\") as file:\n",
    "    json.dump(swordexcercise_output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f872b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c822ad88",
   "metadata": {},
   "source": [
    "# Target Video - Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7259241",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/srirupin/Downloads/target/wave'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f635af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6875e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['layer3']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step    \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach        \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    fully_connected = nn.Linear(256, 512)\n",
    "    output_current_layer = fully_connected(output_current_layer)\n",
    "    layer3_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8d73dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/wave/RATRACE_wave_u_nm_np1_fr_med_9.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a82062ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c62bbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['layer4']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step     \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach        \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    layer4_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b9dc116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/wave/RATRACE_wave_u_nm_np1_fr_med_9.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    frames_collect(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f45d88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "835db409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "#Defining the hook for the layer\n",
    "desired_layers = ['avgpool']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "#Sliding Window Technique with window_size and step    \n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach       \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    \n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    #Transform the pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        #Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            #print(\"The layer is: \", layer_name)\n",
    "            slided_output_layer.append(output)\n",
    "    #Stack the layers\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    avgpool_output.append(output_current_layer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4bec57f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the video /Users/srirupin/Downloads/target/wave/RATRACE_wave_u_nm_np1_fr_med_9.avi\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(input_folder):\n",
    "    video_file_path = os.path.join(input_folder, video_file)\n",
    "    #print(video_file_path)\n",
    "    frames_collect(video_file_path)\n",
    "    #print(video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "878b753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_output =[]\n",
    "wave_output.append(layer3_output)\n",
    "wave_output.append(layer4_output)\n",
    "wave_output.append(avgpool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e54712ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#sword_output = sword_output.tolist()\n",
    "with open(\"wave_output.json\", \"w\") as file:\n",
    "    json.dump(wave_output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b66ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b869b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
