{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a0cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1693683",
   "metadata": {},
   "source": [
    "Replace the video file paths to the location they are at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2617867",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_1 = '/Users/srirupin/Downloads/target/cartwheel/Bodenturnen_2004_cartwheel_f_cm_np1_le_med_0.avi'\n",
    "sample_input_2 = '/Users/srirupin/Downloads/target/sword_exercise/Blade_Of_Fury_-_Scene_1_sword_exercise_f_cm_np1_ri_med_3.avi'\n",
    "sample_input_3 = '/Users/srirupin/Downloads/target/sword/AHF_longsword_against_Rapier_and_Dagger_Fight_sword_f_cm_np2_ri_bad_0.avi'\n",
    "sample_input_4 = '/Users/srirupin/Downloads/target/drink/CastAway2_drink_u_cm_np1_le_goo_8.avi'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5602a478",
   "metadata": {},
   "source": [
    "For each of the sample inputs the function frames_collect() is run to gather the output\n",
    "of each layer. Primarily, the function is ran to gather the output from layer3, layer4 and \n",
    "avgpool layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca67b3e",
   "metadata": {},
   "source": [
    "# Gather the output for layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07cb437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srirupin/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/srirupin/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "     # Gather the output for modules for the specified layers\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    # Registering the hook for the layer in desired_layer\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "\n",
    "# Define layers to capture\n",
    "desired_layers = ['layer3']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "\n",
    "#Sliding Window Technique with window_size and step\n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    #loop through every window of size 32\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach  \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        cv2.imshow(\"The captured frame is: \", frame)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #Defining the maximum frames to 32 and step as 16\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    # Get sequences using sliding window\n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    \n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        # Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    # Stack outputs along a new dimension\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    print(\"The output size: is\",stack_layers.shape)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    fully_connected = nn.Linear(256, 512)\n",
    "    output_current_layer = fully_connected(output_current_layer)\n",
    "    print(\"The output is: \", output_current_layer)\n",
    "    print(\"The output size: is\",output_current_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a594b706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output size: is torch.Size([1, 1, 256, 8, 14, 14])\n",
      "The output is:  tensor([ 0.0990, -0.1084,  0.0919,  0.0403,  0.0946, -0.0779, -0.0827, -0.0630,\n",
      "         0.0057,  0.1921,  0.0034, -0.0213, -0.0260, -0.0489, -0.1136, -0.0471,\n",
      "         0.0205,  0.0069, -0.0073, -0.0289,  0.1373, -0.0526, -0.1285, -0.0909,\n",
      "        -0.0782, -0.0896, -0.0206,  0.0644, -0.1007, -0.1039, -0.0247, -0.0106,\n",
      "        -0.0576,  0.0726, -0.0192, -0.0970, -0.0066,  0.1217,  0.0262, -0.0237,\n",
      "         0.1730,  0.0543, -0.0304,  0.0298, -0.0322, -0.0617, -0.0817, -0.0462,\n",
      "        -0.0892, -0.0161, -0.0046,  0.0825,  0.1646,  0.0720, -0.0878, -0.0814,\n",
      "         0.1157,  0.0181,  0.0241, -0.0275,  0.0079,  0.0264, -0.0884,  0.0378,\n",
      "         0.0361,  0.0472,  0.0048,  0.1543,  0.1070,  0.0133,  0.0558,  0.0068,\n",
      "        -0.0335,  0.0975,  0.0118, -0.0207,  0.0047,  0.0258, -0.1178,  0.0730,\n",
      "         0.0428,  0.0471, -0.0853, -0.0902,  0.0585, -0.0584, -0.1048,  0.0268,\n",
      "         0.0822, -0.0505,  0.0702, -0.0164, -0.0503, -0.0775, -0.0167,  0.0331,\n",
      "        -0.0342, -0.0597,  0.1328,  0.0500,  0.0164, -0.1344, -0.0243, -0.0307,\n",
      "        -0.1597,  0.0738,  0.0021,  0.0294,  0.0580, -0.0106, -0.0330,  0.0663,\n",
      "         0.0110, -0.0362, -0.0175,  0.0035, -0.0187,  0.0145,  0.0960, -0.0742,\n",
      "        -0.0557, -0.0443, -0.0853,  0.0670,  0.0347, -0.0223, -0.0780,  0.0543,\n",
      "         0.0714,  0.0271,  0.0168,  0.0281,  0.0494,  0.0036,  0.0184, -0.0361,\n",
      "         0.0437,  0.0332,  0.0594, -0.0149, -0.1288, -0.1211, -0.0801,  0.0366,\n",
      "         0.0533,  0.0371,  0.0888,  0.0406, -0.0448, -0.0112,  0.0949, -0.0732,\n",
      "        -0.0257, -0.0604,  0.0480,  0.0801, -0.1352,  0.0281,  0.0477,  0.0076,\n",
      "         0.0965,  0.0707, -0.0488,  0.0354,  0.0257, -0.0466, -0.0276, -0.1864,\n",
      "         0.1265, -0.1269, -0.0408,  0.1008,  0.0932, -0.0040,  0.0215, -0.0510,\n",
      "        -0.0060,  0.0692,  0.0235, -0.1084,  0.0578,  0.0706,  0.1204, -0.0517,\n",
      "        -0.0547,  0.0835,  0.0929,  0.0791, -0.0709,  0.1409, -0.0012, -0.0231,\n",
      "         0.0800, -0.0415,  0.0200,  0.0109,  0.0447, -0.0228,  0.0697, -0.0500,\n",
      "        -0.0745, -0.0017,  0.0471, -0.0224,  0.0099, -0.1412,  0.0090,  0.0344,\n",
      "        -0.0458,  0.0482, -0.0371,  0.0320,  0.0574, -0.0953, -0.0777, -0.0527,\n",
      "         0.0472, -0.0336, -0.0287, -0.0129, -0.0501, -0.0352, -0.0791,  0.0098,\n",
      "        -0.0077, -0.0125,  0.0569, -0.0640, -0.0154, -0.0093, -0.0182, -0.0541,\n",
      "         0.1014, -0.0291,  0.0006,  0.0793,  0.1390, -0.0868,  0.1668, -0.0769,\n",
      "        -0.0543,  0.0331, -0.0302,  0.0845, -0.0343, -0.0450, -0.0061,  0.0132,\n",
      "         0.0006, -0.0035, -0.0138, -0.0020,  0.0470, -0.0993, -0.0277,  0.0775,\n",
      "         0.0389, -0.0275, -0.0536,  0.0529, -0.0213,  0.0576,  0.0960,  0.0521,\n",
      "        -0.0927,  0.0196, -0.0675,  0.0029, -0.0075, -0.1473,  0.1724,  0.0103,\n",
      "         0.0484, -0.2754, -0.0590, -0.0819, -0.0531,  0.0395,  0.1050, -0.0774,\n",
      "        -0.1088, -0.0459,  0.1269, -0.0604,  0.0765,  0.0290,  0.0725,  0.0318,\n",
      "        -0.0105,  0.0612,  0.1715, -0.1073,  0.0679,  0.0048,  0.0729, -0.0260,\n",
      "         0.1360, -0.0339, -0.0574,  0.0543, -0.0772,  0.1070,  0.0459, -0.0755,\n",
      "         0.0243,  0.0715,  0.2064,  0.0299, -0.0607,  0.0146,  0.0618,  0.0512,\n",
      "        -0.0409,  0.0301, -0.0197,  0.0054,  0.0394,  0.0533,  0.1000, -0.0882,\n",
      "        -0.0954,  0.0336,  0.0050,  0.1002,  0.0083,  0.1180, -0.0710,  0.0920,\n",
      "        -0.1510, -0.0488, -0.0486,  0.1136, -0.0788,  0.0247,  0.0828,  0.0750,\n",
      "         0.0014, -0.0968,  0.0258,  0.0033,  0.0470, -0.0316,  0.0010,  0.0010,\n",
      "        -0.1242,  0.0131, -0.0409, -0.0981,  0.0277, -0.0047, -0.0956,  0.0321,\n",
      "         0.0031, -0.0226, -0.0428, -0.0879, -0.0756,  0.1012,  0.0663, -0.0405,\n",
      "         0.0200, -0.0534, -0.0004, -0.0098, -0.0832, -0.0249,  0.1375, -0.0198,\n",
      "        -0.0353,  0.0945,  0.1129,  0.0563,  0.0181,  0.0173,  0.0389, -0.0780,\n",
      "         0.1057, -0.1064, -0.0477,  0.0349,  0.0219, -0.0459,  0.0547,  0.1849,\n",
      "         0.0656, -0.0835, -0.0399,  0.0281, -0.0412,  0.0311, -0.0215, -0.0339,\n",
      "        -0.1481,  0.0157,  0.0114,  0.0517,  0.0781,  0.0051, -0.0945, -0.0898,\n",
      "         0.0301,  0.0902,  0.0563, -0.1083,  0.1170,  0.0412, -0.0078,  0.0247,\n",
      "         0.0689,  0.0237,  0.0265,  0.0174, -0.0904, -0.0195,  0.0256, -0.0909,\n",
      "         0.0307, -0.0553,  0.0659, -0.1825,  0.0346,  0.0064, -0.0202, -0.0483,\n",
      "        -0.0884,  0.0610, -0.0127, -0.0359, -0.0617, -0.1070,  0.0704, -0.1242,\n",
      "         0.1155, -0.0590, -0.0629,  0.0218,  0.0596,  0.0076,  0.0184, -0.0613,\n",
      "        -0.0526, -0.1216,  0.0195,  0.0952, -0.0455,  0.0135,  0.0138, -0.0802,\n",
      "        -0.0142, -0.0387,  0.1155, -0.0054, -0.0093,  0.0936, -0.0112, -0.0073,\n",
      "        -0.0410,  0.0050, -0.0332, -0.0074,  0.0370, -0.0231, -0.0109,  0.0055,\n",
      "         0.0629,  0.1087, -0.0632,  0.0116, -0.0624,  0.1181,  0.0612,  0.0116,\n",
      "        -0.1960, -0.0472,  0.0421,  0.1105, -0.0787, -0.1411, -0.0221,  0.1294,\n",
      "        -0.0337,  0.0083, -0.1139, -0.0836, -0.1107, -0.0270,  0.0035, -0.0273,\n",
      "        -0.0947,  0.0319, -0.0284,  0.0339, -0.0493, -0.0471, -0.1077,  0.0761,\n",
      "        -0.1305,  0.0462,  0.0971,  0.0158, -0.0177,  0.0521, -0.0249,  0.1208,\n",
      "        -0.1531, -0.0293,  0.0875,  0.0764, -0.1093,  0.0299,  0.0060, -0.1268],\n",
      "       grad_fn=<AddBackward0>)\n",
      "The output size: is torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561147d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output size: is torch.Size([5, 1, 256, 8, 14, 14])\n",
      "The output is:  tensor([ 1.4120e-02,  1.3162e-01,  1.2358e-01,  9.4904e-03,  5.8576e-02,\n",
      "        -6.3274e-03, -1.4801e-01,  4.7116e-04,  1.7889e-01,  8.9640e-02,\n",
      "         1.6909e-02, -1.4039e-01, -7.7162e-02,  3.4133e-02, -8.7308e-03,\n",
      "        -1.0690e-01, -1.6448e-01, -1.9020e-01, -1.3194e-01, -9.1860e-02,\n",
      "         1.5554e-02, -1.3204e-01,  7.8106e-02,  1.6684e-02, -2.3045e-01,\n",
      "         2.4443e-02,  2.3388e-02,  1.1729e-02, -1.1378e-01, -5.5132e-02,\n",
      "        -1.8625e-01, -2.0549e-01,  8.0442e-03,  1.5091e-01, -1.0420e-01,\n",
      "         5.8827e-02, -1.1247e-01,  6.7035e-02,  3.2475e-01,  1.4229e-01,\n",
      "         9.3153e-02,  6.2190e-02, -2.7965e-01, -1.2180e-01, -5.9984e-02,\n",
      "         2.4322e-01, -1.2436e-01, -3.0442e-02,  1.7546e-03, -6.8871e-02,\n",
      "        -4.2046e-03,  5.5112e-02, -2.5628e-02,  2.2816e-01,  1.0023e-01,\n",
      "        -9.5258e-02,  3.6586e-02, -5.0268e-02,  2.6501e-02,  2.9987e-02,\n",
      "         1.2623e-01, -4.6841e-02,  7.9813e-02, -2.1724e-02, -1.2015e-01,\n",
      "         3.7009e-02,  3.2291e-02, -1.2592e-01,  6.8678e-02, -2.4300e-02,\n",
      "        -5.7377e-02,  9.5617e-02,  8.1808e-02,  1.6322e-02, -6.2550e-03,\n",
      "         3.9259e-03, -8.8015e-02,  7.4308e-02, -3.8090e-03, -2.7369e-01,\n",
      "         1.2914e-01, -1.0175e-01, -6.8209e-02,  2.8601e-01,  7.8117e-02,\n",
      "         7.0339e-02,  1.7056e-01,  1.7354e-02,  4.4843e-02, -6.7146e-02,\n",
      "        -1.4524e-01, -6.0784e-02, -5.3894e-02, -1.1509e-01, -1.1385e-01,\n",
      "        -7.6080e-02, -1.4956e-01,  1.1612e-01,  6.4726e-02,  1.7500e-01,\n",
      "        -2.3159e-01, -6.1989e-02, -2.2956e-02, -8.5476e-02,  3.9227e-02,\n",
      "        -1.1897e-03, -1.0106e-02, -2.1279e-01,  4.9960e-02,  1.1930e-03,\n",
      "        -1.1984e-01, -7.5843e-02,  1.2952e-02,  7.2972e-04, -6.8209e-02,\n",
      "         7.9048e-02, -2.3696e-02, -1.3418e-01, -2.2795e-02, -7.3694e-02,\n",
      "         1.4774e-03, -1.0587e-01,  1.8214e-01,  7.2470e-02,  1.0580e-01,\n",
      "        -1.2815e-02,  1.0832e-01,  1.0497e-01,  2.0115e-01, -1.4978e-01,\n",
      "        -6.7261e-02,  2.8856e-01,  1.0485e-01,  7.6058e-02,  1.0753e-01,\n",
      "         9.7271e-02, -6.1476e-02, -1.8552e-01,  2.7860e-03,  7.0027e-02,\n",
      "         8.5285e-02,  3.0549e-01, -1.1989e-01,  6.2932e-02, -5.2387e-02,\n",
      "        -5.2516e-02,  1.4617e-01, -9.5899e-02,  8.1542e-02,  1.4701e-02,\n",
      "         1.3341e-01,  6.0231e-02,  2.0043e-01,  2.6747e-01, -5.1878e-05,\n",
      "        -9.2585e-02,  2.1146e-01, -1.4440e-01,  1.0022e-01,  2.9385e-02,\n",
      "        -1.6588e-01,  7.2849e-02, -4.9117e-02,  9.3014e-02,  1.5224e-01,\n",
      "         2.4591e-02,  2.5325e-01, -2.1910e-01,  1.5044e-01,  9.1877e-02,\n",
      "        -8.8303e-02, -1.0038e-01,  1.0222e-01, -1.3904e-01,  1.2049e-01,\n",
      "         1.0846e-01, -4.3969e-02, -2.5195e-01, -5.8793e-02, -6.3846e-02,\n",
      "        -7.7765e-02,  2.0634e-02, -5.7117e-02,  1.1944e-02,  2.5071e-02,\n",
      "         1.0053e-01, -1.7956e-01, -7.5065e-02,  6.5385e-03, -6.0268e-02,\n",
      "        -1.0364e-01, -1.2024e-01,  1.3785e-01,  2.9807e-02, -1.3074e-01,\n",
      "        -1.7219e-01, -1.2089e-01,  2.1760e-01, -5.6380e-02, -3.4477e-02,\n",
      "        -2.6054e-02, -7.4493e-02, -1.2830e-01,  1.1221e-01, -1.0327e-01,\n",
      "        -1.8899e-02, -5.9530e-02,  9.0445e-02,  1.4895e-01,  7.2971e-02,\n",
      "         1.6155e-01,  1.7271e-01,  1.3351e-01, -5.0229e-02, -1.6328e-02,\n",
      "         2.5768e-01,  4.3775e-02, -1.6459e-01,  4.3436e-02, -3.9738e-02,\n",
      "        -2.0941e-01,  1.7406e-01,  5.6969e-02, -2.6797e-01,  2.6580e-03,\n",
      "        -5.0857e-02,  1.2762e-01,  1.2987e-01,  2.0943e-02, -6.9040e-02,\n",
      "        -1.3354e-01, -1.0766e-02,  9.5133e-02,  6.8620e-02,  1.4586e-01,\n",
      "        -7.4897e-02,  1.9824e-01,  3.0701e-01,  3.5520e-02, -2.1060e-01,\n",
      "        -1.6523e-01, -1.0945e-01, -2.2360e-01, -2.9409e-02,  6.0979e-02,\n",
      "         1.1334e-01, -5.1947e-02,  4.2510e-02, -1.6023e-02,  7.8130e-02,\n",
      "        -3.5280e-02, -1.4013e-01,  5.6136e-02,  6.7700e-02,  9.4619e-02,\n",
      "         4.7566e-02, -4.1794e-02, -8.0415e-02, -3.0977e-02, -6.2932e-02,\n",
      "        -4.0388e-02, -8.7439e-02,  1.4067e-01,  1.2663e-01,  7.5114e-02,\n",
      "        -1.0804e-02, -4.9696e-03, -3.5959e-02,  1.9656e-01,  7.4864e-02,\n",
      "        -1.0159e-02, -8.0143e-02,  1.7433e-01,  1.8101e-01, -4.5231e-02,\n",
      "         4.0906e-02,  1.8691e-02,  2.0000e-01,  2.1536e-01, -1.5177e-01,\n",
      "        -6.6196e-02, -6.7599e-02,  2.7413e-02,  3.3408e-01,  1.1857e-01,\n",
      "        -4.4612e-02,  7.4594e-02,  1.4828e-02, -8.7044e-02,  5.9765e-02,\n",
      "         1.1381e-01, -1.8309e-02,  5.0216e-02, -8.0421e-02, -9.1007e-02,\n",
      "         1.8133e-02, -9.2210e-02, -3.9157e-02, -1.0798e-01,  5.1941e-02,\n",
      "        -4.8109e-02,  5.0393e-03,  9.2888e-02, -1.1349e-03,  1.6026e-02,\n",
      "        -1.0842e-01, -3.2923e-02,  1.5962e-02, -1.1527e-01, -7.0771e-02,\n",
      "         1.0190e-01, -5.7820e-02, -2.3031e-02, -2.1881e-01,  1.8146e-01,\n",
      "         1.0196e-01,  1.6820e-01,  6.0251e-02,  2.0544e-02, -1.7315e-02,\n",
      "        -5.0791e-02,  8.0284e-03,  1.3845e-01,  1.0540e-01,  1.3117e-01,\n",
      "         6.5692e-02,  1.5920e-02, -7.9974e-03, -5.0396e-02, -9.0249e-02,\n",
      "         1.6408e-01, -2.1239e-01, -3.3204e-02,  2.8198e-01, -7.5748e-02,\n",
      "         2.0723e-01, -2.2371e-02, -1.3622e-01,  3.6852e-02, -6.6476e-02,\n",
      "        -1.0978e-01, -5.8745e-03,  8.0651e-02,  4.3680e-03,  6.6683e-02,\n",
      "         7.4951e-02,  2.8138e-01, -1.4096e-01, -1.6669e-01,  7.3969e-02,\n",
      "         5.0365e-02, -3.8143e-04,  1.2488e-01, -1.8663e-01,  5.7547e-02,\n",
      "         1.9412e-02, -1.7233e-01, -1.5293e-01,  1.5150e-01,  1.3980e-01,\n",
      "         5.4917e-03, -1.4534e-01,  8.6827e-03, -4.5851e-02, -5.1270e-02,\n",
      "         1.8044e-01,  5.7457e-02, -3.2134e-02, -3.5079e-02, -1.8509e-02,\n",
      "         6.0726e-02,  1.0848e-01,  8.3477e-02,  1.0867e-01, -1.6466e-02,\n",
      "         4.3629e-03, -4.4484e-02,  4.2273e-02,  6.5725e-03,  3.1701e-01,\n",
      "        -3.0002e-02, -5.7241e-02,  2.7258e-01,  1.6433e-01,  9.8513e-02,\n",
      "         1.2555e-01, -5.0057e-02, -6.1862e-02, -8.3161e-02, -7.1659e-02,\n",
      "         3.8056e-02, -8.5403e-02, -5.6283e-02, -1.7398e-01,  1.4181e-02,\n",
      "         7.7344e-02, -7.7823e-02,  7.8857e-02, -1.2477e-01, -7.3004e-02,\n",
      "         8.9146e-02,  1.0169e-01,  1.2618e-02,  1.0819e-01,  3.8574e-02,\n",
      "         7.1553e-02,  1.8631e-01, -8.4381e-02, -1.9198e-02,  1.8558e-01,\n",
      "         1.3408e-01, -2.8015e-02, -1.9283e-02,  1.1315e-01,  2.1945e-01,\n",
      "         4.8523e-02,  1.9479e-01,  9.3274e-02,  1.4655e-01,  2.6221e-02,\n",
      "        -1.6414e-01, -2.3530e-02, -7.0334e-02, -2.2188e-02,  3.8991e-02,\n",
      "        -4.3091e-02,  5.3744e-02,  2.5893e-02, -1.8180e-01,  1.9889e-01,\n",
      "        -2.9056e-01, -8.9936e-03, -4.8062e-02, -1.3314e-01, -4.5445e-02,\n",
      "         1.2186e-01, -1.7260e-01, -1.2605e-01,  9.2030e-02, -3.5617e-02,\n",
      "        -7.1135e-02, -1.0983e-01,  5.3428e-03, -7.2822e-02, -3.9827e-01,\n",
      "         9.9421e-02,  2.2450e-01,  1.6767e-01,  1.2662e-02,  1.0155e-02,\n",
      "         1.6418e-01,  7.0783e-02,  9.5204e-02,  7.2425e-02, -1.0102e-01,\n",
      "         3.2599e-02,  9.9593e-02,  2.0561e-02, -4.0267e-02,  5.6959e-02,\n",
      "         1.0507e-01,  2.7707e-01,  8.5811e-02,  6.9518e-02,  5.1305e-02,\n",
      "         1.2977e-01, -2.5557e-02, -1.4702e-01,  9.3866e-02, -1.9420e-02,\n",
      "         7.3444e-02, -4.4243e-02,  1.2163e-01,  6.1711e-02,  9.6318e-02,\n",
      "        -1.6363e-02, -1.7779e-01,  1.1331e-02, -7.8483e-02, -1.3227e-01,\n",
      "        -1.4263e-01,  9.1451e-02, -6.8484e-02, -1.0220e-01, -1.0987e-01,\n",
      "        -6.9683e-03, -8.7323e-02,  6.2721e-02, -8.1744e-02,  2.6923e-02,\n",
      "        -7.5381e-02,  1.0601e-01,  1.4685e-02,  3.7053e-02,  1.2859e-01,\n",
      "        -1.1564e-01,  7.6525e-02, -9.8594e-02, -1.1778e-02,  1.2366e-01,\n",
      "         1.3006e-02,  1.4870e-01, -9.8946e-02, -1.0360e-02,  1.0860e-01,\n",
      "         1.9308e-01,  1.4315e-01, -6.5263e-02, -2.0945e-01,  1.4671e-01,\n",
      "        -6.0136e-02, -1.2825e-02], grad_fn=<AddBackward0>)\n",
      "The output size: is torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252dff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output size: is torch.Size([1, 1, 256, 8, 14, 14])\n",
      "The output is:  tensor([ 8.1665e-02,  3.4364e-02,  6.1930e-02,  9.1873e-03,  4.9294e-02,\n",
      "        -5.1080e-02, -9.9974e-02, -2.7652e-02, -2.8828e-02,  7.5942e-02,\n",
      "         5.4733e-02, -1.2082e-01, -4.3095e-02,  7.4740e-03, -2.4453e-02,\n",
      "         6.8239e-02,  1.0285e-01, -6.1899e-02, -7.7998e-03, -5.7646e-02,\n",
      "         8.9190e-02, -2.4081e-03,  7.6920e-02, -1.2542e-01, -9.0308e-02,\n",
      "         4.3935e-02, -6.1942e-02, -1.2831e-02,  3.3199e-02,  1.3606e-01,\n",
      "        -6.9822e-02, -6.9676e-02, -5.0564e-02,  2.4191e-02, -6.1515e-02,\n",
      "         3.1978e-02,  5.5891e-02,  3.8564e-02, -9.3211e-03,  2.6767e-02,\n",
      "        -4.7748e-02, -1.3109e-02,  8.1868e-03, -4.0652e-02, -1.3115e-01,\n",
      "        -1.1526e-01, -2.2859e-02,  3.0477e-02, -1.2870e-01, -4.5798e-03,\n",
      "        -3.2916e-02,  3.7236e-02,  4.5703e-02,  5.1948e-02,  1.1599e-01,\n",
      "        -1.5780e-02, -1.8405e-02,  1.8339e-01, -1.2520e-01,  6.5151e-02,\n",
      "         7.2755e-02,  2.0302e-02,  6.5847e-03,  6.4440e-03, -4.1771e-02,\n",
      "         4.3450e-02,  1.3501e-01,  3.6572e-02,  1.9236e-01,  2.1254e-02,\n",
      "        -1.6173e-01,  2.4175e-03, -4.2410e-02, -1.7901e-02,  1.1452e-02,\n",
      "        -3.4313e-02, -2.9987e-02,  1.2565e-01,  1.0492e-01, -1.2422e-03,\n",
      "        -2.9822e-03, -5.3347e-02,  1.0291e-02, -8.0308e-02, -9.3002e-03,\n",
      "         8.6453e-02, -7.8116e-02,  9.0722e-02, -3.6792e-02,  4.3067e-02,\n",
      "        -5.2469e-02,  1.1152e-01,  8.4980e-02, -1.0044e-01, -7.4390e-03,\n",
      "         1.6665e-02, -4.7619e-02, -1.1467e-01,  1.1013e-02, -3.3385e-02,\n",
      "        -3.0550e-02, -1.8092e-02, -5.0100e-02, -3.4096e-02, -4.1175e-02,\n",
      "         3.6478e-02,  1.2917e-01,  6.4261e-02,  6.7675e-02,  1.1830e-01,\n",
      "        -2.1106e-02,  8.1788e-02,  7.9757e-02, -8.6300e-02, -9.8776e-02,\n",
      "         8.7450e-02, -2.8495e-02, -2.3683e-02, -4.9432e-02,  1.0528e-01,\n",
      "        -1.2838e-02, -2.4753e-02,  4.5909e-02,  1.2846e-01,  3.5744e-02,\n",
      "        -1.6745e-02,  7.6111e-02, -1.0865e-01, -4.5359e-02,  1.0349e-01,\n",
      "        -7.8614e-02, -1.0726e-01, -8.7239e-02,  6.7642e-02, -8.8010e-02,\n",
      "         1.3882e-01, -3.6041e-02,  4.2670e-02,  5.0726e-02, -7.6526e-02,\n",
      "        -1.4089e-01,  3.6492e-02, -5.9938e-02, -1.1642e-01,  1.1851e-02,\n",
      "         2.7837e-02, -4.5861e-02, -5.1231e-02, -6.7507e-02,  5.2342e-02,\n",
      "         8.3543e-03, -4.8456e-02,  6.6712e-03,  6.5427e-03,  1.4843e-01,\n",
      "         1.4937e-01,  1.2413e-03, -3.9638e-03, -1.1008e-01, -1.3842e-02,\n",
      "         5.7266e-02,  5.4062e-02, -2.4421e-02,  1.4366e-01,  1.5896e-02,\n",
      "         9.4334e-02, -5.0940e-03, -5.1884e-02,  3.1404e-02,  7.3712e-02,\n",
      "         2.9828e-02,  1.1042e-01,  5.3765e-03,  1.0485e-01,  7.5329e-02,\n",
      "        -2.4717e-02, -5.9623e-02,  9.8422e-02, -1.3003e-02, -2.4616e-02,\n",
      "        -2.8075e-02, -2.5657e-03, -4.6017e-02,  1.2443e-01, -1.4168e-01,\n",
      "         1.2488e-02, -2.6878e-02, -6.5617e-02, -8.9750e-02, -6.3994e-02,\n",
      "        -3.6298e-02,  1.7995e-03,  2.0536e-02,  3.8757e-02, -3.9689e-02,\n",
      "         7.0644e-02,  4.2778e-02,  3.6616e-02, -1.0628e-01,  3.5626e-02,\n",
      "         3.5869e-02,  5.8857e-02,  2.3481e-02,  3.5006e-02, -3.1145e-02,\n",
      "         6.5705e-03, -6.1862e-02, -4.8067e-02,  7.6812e-02,  7.8771e-04,\n",
      "         1.4617e-01,  5.6847e-03, -2.7420e-02, -2.3487e-02, -1.5037e-01,\n",
      "        -8.1217e-03,  7.2665e-02, -1.0853e-02,  1.8169e-02,  1.1930e-01,\n",
      "         1.3183e-01, -1.1252e-02, -1.1178e-01,  2.2932e-03, -7.1142e-02,\n",
      "         3.9758e-02,  8.3817e-02,  6.7177e-02, -9.4727e-04, -6.7684e-02,\n",
      "         2.6206e-01, -1.0450e-01, -8.5424e-02,  6.6989e-02, -1.2455e-01,\n",
      "         2.7678e-02,  3.0262e-02, -1.1754e-01,  8.7685e-02, -5.9423e-02,\n",
      "        -3.4238e-02,  1.0713e-01,  3.1208e-02,  5.1980e-02,  1.3713e-02,\n",
      "         7.9950e-02,  2.1214e-02, -1.9508e-03, -3.4488e-02, -1.1189e-01,\n",
      "        -8.8447e-02,  8.1946e-02, -3.9939e-02, -1.0223e-01, -1.1661e-01,\n",
      "        -1.8095e-02,  2.9176e-02,  1.1064e-01,  6.7566e-02,  1.5469e-01,\n",
      "        -2.0606e-02,  3.7700e-02, -1.0746e-02,  7.4509e-02,  8.6542e-02,\n",
      "         8.3785e-02, -2.8283e-02,  1.0863e-01, -2.1853e-03,  1.3113e-02,\n",
      "         7.6932e-02, -4.9755e-02, -6.9005e-03, -4.6654e-02, -5.0670e-02,\n",
      "        -1.4710e-01, -6.6378e-02,  6.6310e-02, -9.0861e-02,  3.0895e-02,\n",
      "         1.1103e-01, -8.0803e-03,  8.4394e-02,  9.1438e-02,  5.6850e-02,\n",
      "         1.1300e-01, -1.7263e-01,  9.4031e-02,  5.8920e-02, -6.6120e-02,\n",
      "        -9.0976e-02,  2.4714e-02, -2.4490e-02, -1.3990e-02, -1.3301e-01,\n",
      "         1.1671e-01, -1.3127e-01, -3.4650e-03, -1.0583e-01, -2.7756e-02,\n",
      "        -6.2608e-02,  7.2708e-03,  3.7623e-02, -7.9465e-02, -6.8477e-02,\n",
      "        -6.6154e-02,  5.3612e-02,  3.5211e-02,  4.6143e-02,  2.4753e-03,\n",
      "         5.3010e-02, -5.0474e-02,  1.9982e-02,  7.3293e-02,  6.4579e-02,\n",
      "        -5.5611e-02, -3.5337e-03, -2.0024e-02, -1.5882e-02,  1.0968e-01,\n",
      "        -5.5953e-03,  5.3707e-02,  1.3347e-02, -9.1969e-02, -1.6688e-01,\n",
      "        -4.8926e-02,  1.9141e-02, -5.1921e-02, -7.0297e-02, -9.5606e-02,\n",
      "        -4.6243e-02,  9.0563e-02, -1.5978e-02,  2.2737e-02,  2.2844e-02,\n",
      "         8.6655e-02, -5.2829e-03, -3.4141e-03,  8.3811e-03, -8.5409e-02,\n",
      "         7.4692e-02, -1.6771e-02, -8.1897e-02, -5.5197e-02, -1.8147e-02,\n",
      "         8.4901e-02, -4.9174e-02, -2.6386e-02, -2.5691e-02, -5.6205e-02,\n",
      "         2.4822e-02, -1.1288e-01, -4.7704e-02,  3.8671e-02, -2.3447e-02,\n",
      "        -4.1812e-03, -9.6353e-02, -1.9520e-01,  5.9293e-03, -1.9090e-02,\n",
      "        -9.7326e-02, -1.0697e-02, -4.2095e-02,  3.0020e-03, -1.0774e-01,\n",
      "        -3.1278e-02,  8.7352e-02, -2.3732e-02,  3.1370e-02, -1.3325e-01,\n",
      "        -4.5805e-02, -1.9914e-01, -9.7806e-02, -6.3190e-02, -2.7834e-02,\n",
      "         1.1665e-01,  1.7024e-01,  9.1031e-02, -1.1388e-01,  3.6850e-02,\n",
      "        -9.6242e-02,  4.0650e-02,  1.8132e-02, -1.2211e-01, -9.1900e-02,\n",
      "        -3.4325e-03,  1.2198e-01,  8.9613e-02, -2.0946e-02, -2.6666e-02,\n",
      "        -4.3037e-02,  1.0505e-02, -1.9729e-01,  6.1243e-02, -3.4472e-02,\n",
      "         1.1795e-02, -5.2473e-02, -5.1836e-02,  5.7547e-02,  5.6620e-02,\n",
      "        -5.3965e-02,  6.7494e-02, -4.9001e-02,  1.1750e-03,  1.0555e-01,\n",
      "        -5.1375e-02, -1.3364e-02,  2.9600e-02,  6.7098e-02, -3.1319e-02,\n",
      "        -4.1953e-04,  6.7838e-02,  5.6322e-02, -5.7960e-02,  5.7250e-02,\n",
      "        -9.6397e-02,  5.8997e-03, -5.3306e-02, -8.4409e-03, -2.8495e-02,\n",
      "        -8.7463e-02,  9.0922e-02, -8.1401e-02, -2.0387e-02, -5.2228e-02,\n",
      "        -2.3107e-03, -4.7563e-02, -5.8995e-02, -6.1873e-02,  1.6146e-02,\n",
      "         1.2897e-01, -2.4925e-02,  4.3583e-02,  1.0190e-02,  1.3237e-03,\n",
      "         1.2312e-01, -1.5364e-02,  1.0485e-01,  5.0943e-02, -9.4855e-02,\n",
      "         6.5616e-02,  5.9275e-02,  9.0352e-03, -3.5218e-02,  5.8639e-02,\n",
      "         1.4792e-01,  5.6938e-02, -3.3726e-02,  1.6285e-01, -1.7679e-02,\n",
      "         8.4741e-02, -1.2771e-02,  4.0258e-02, -3.3289e-02,  1.0039e-01,\n",
      "        -3.5123e-02,  2.0647e-02,  9.1765e-02,  4.0276e-02,  4.3332e-02,\n",
      "        -6.4483e-02, -3.1181e-02,  1.3852e-02,  4.3367e-02, -4.1633e-02,\n",
      "         5.2638e-02, -5.7044e-02, -1.0540e-01, -8.3916e-03,  9.4575e-02,\n",
      "        -6.6787e-02,  3.5555e-02, -2.2293e-02,  1.2696e-01,  1.4519e-02,\n",
      "        -7.5880e-03,  1.9112e-01,  1.1806e-01,  5.5347e-02,  2.5727e-02,\n",
      "         7.2343e-03, -6.2845e-03, -2.0670e-04,  4.9381e-02, -6.2618e-02,\n",
      "        -9.7723e-02,  1.4039e-02, -1.9352e-02, -5.7379e-02,  3.3907e-02,\n",
      "        -9.3702e-02, -2.2952e-02,  5.7144e-02, -6.7220e-02,  1.5910e-02,\n",
      "        -1.3140e-01, -8.2198e-02,  5.7203e-02, -1.2036e-01,  3.2562e-02,\n",
      "        -1.5028e-02, -1.4930e-01,  4.6373e-02, -7.1129e-02,  1.4676e-02,\n",
      "         5.2938e-02,  2.9249e-02, -2.0359e-02, -1.1211e-01,  6.4750e-02,\n",
      "        -7.7782e-02, -5.5432e-02], grad_fn=<AddBackward0>)\n",
      "The output size: is torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13780598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output size: is torch.Size([9, 1, 256, 8, 14, 14])\n",
      "The output is:  tensor([-0.2307, -0.0642, -0.0689, -0.1007, -0.2788,  0.1421,  0.1256,  0.0864,\n",
      "        -0.0403,  0.1457,  0.1639,  0.3238, -0.0476, -0.3572,  0.2370, -0.0010,\n",
      "         0.1660, -0.1253, -0.1086, -0.1417, -0.2164, -0.0291,  0.0639, -0.1175,\n",
      "         0.3338,  0.2228, -0.0900, -0.2499, -0.0356,  0.2439, -0.0643, -0.1139,\n",
      "         0.0230, -0.1626,  0.1055,  0.1143,  0.1869, -0.0109,  0.0686,  0.1791,\n",
      "        -0.0630, -0.0791,  0.1917,  0.0611, -0.1312,  0.1573, -0.1952, -0.1504,\n",
      "         0.1373, -0.1132, -0.1429,  0.0370,  0.1789,  0.1670,  0.2650,  0.0116,\n",
      "        -0.0111, -0.2472,  0.0112, -0.0300,  0.2424, -0.0268, -0.0276, -0.0553,\n",
      "         0.1722, -0.0864, -0.1903,  0.0412, -0.0608, -0.0573, -0.2163, -0.0894,\n",
      "        -0.1644, -0.0783, -0.1227,  0.1677,  0.2340,  0.4091, -0.2211,  0.0345,\n",
      "         0.0750, -0.0853,  0.1347, -0.0926,  0.0826,  0.0567, -0.0686,  0.0645,\n",
      "         0.1049, -0.0031,  0.3269,  0.0791, -0.3848, -0.0279, -0.1566, -0.1728,\n",
      "        -0.3799,  0.2506, -0.0021, -0.0074,  0.1402, -0.2056, -0.0027, -0.1644,\n",
      "        -0.3223,  0.1931, -0.0743, -0.0021,  0.3246, -0.1152, -0.2703, -0.0115,\n",
      "         0.0255,  0.0589,  0.1062, -0.1904, -0.0728, -0.4035,  0.0439, -0.2338,\n",
      "         0.0017,  0.0345, -0.1114, -0.1090, -0.0441, -0.3057,  0.0311,  0.1625,\n",
      "         0.0100, -0.1029, -0.1568, -0.1945, -0.3436,  0.0531, -0.0323, -0.1858,\n",
      "        -0.2561,  0.2809,  0.2432, -0.1512, -0.1041, -0.1055, -0.1468,  0.0444,\n",
      "         0.1779,  0.0110,  0.1746, -0.0239,  0.1138,  0.2082,  0.0324, -0.0398,\n",
      "         0.1468, -0.2564, -0.1979, -0.2840,  0.2628, -0.1703,  0.0347,  0.0101,\n",
      "         0.2986, -0.2409,  0.3547,  0.1795, -0.0396,  0.0989, -0.2763,  0.0739,\n",
      "        -0.0390,  0.2157,  0.3046, -0.0623,  0.1942,  0.2065, -0.1237, -0.0079,\n",
      "         0.0326, -0.3428,  0.0704,  0.1211,  0.1388, -0.0502, -0.0900, -0.0997,\n",
      "         0.0223, -0.0208,  0.0669,  0.1718, -0.1174, -0.1808,  0.2445,  0.2229,\n",
      "         0.0832, -0.0630, -0.0840, -0.0018, -0.2689, -0.0275,  0.0030,  0.1881,\n",
      "        -0.1035, -0.2291,  0.0510,  0.1445,  0.1048,  0.4432,  0.0376, -0.1222,\n",
      "        -0.0634, -0.0641,  0.0927,  0.3872, -0.1663, -0.0936,  0.1007,  0.0255,\n",
      "        -0.0844,  0.0404,  0.3126,  0.0498, -0.0350, -0.3603, -0.1761,  0.0276,\n",
      "        -0.0196, -0.1013, -0.0892,  0.3774, -0.2809,  0.0260, -0.1310,  0.2013,\n",
      "         0.0816, -0.1130, -0.2212, -0.1780,  0.4179,  0.0738,  0.1444,  0.3564,\n",
      "         0.0423, -0.0596,  0.1706, -0.0414,  0.0362,  0.1115,  0.0197, -0.1114,\n",
      "         0.1224, -0.1251,  0.2231, -0.3379,  0.0760, -0.2437, -0.1671,  0.1951,\n",
      "         0.2008, -0.0911, -0.0263,  0.1902,  0.1632,  0.3049,  0.1624,  0.0733,\n",
      "        -0.1612, -0.0798,  0.0391, -0.0111, -0.2041,  0.1006,  0.1559,  0.0190,\n",
      "        -0.2997, -0.2450,  0.0927,  0.1256, -0.0053, -0.0674,  0.0532, -0.0848,\n",
      "         0.3140, -0.2398, -0.0868,  0.1606,  0.0726, -0.2087,  0.0594,  0.0107,\n",
      "         0.0279,  0.1578,  0.1472,  0.2093,  0.1439, -0.1920,  0.0523, -0.2058,\n",
      "        -0.0413, -0.1124, -0.2160,  0.1370,  0.0213,  0.0420, -0.2099,  0.0930,\n",
      "        -0.0768, -0.2846,  0.0730,  0.2562,  0.0698, -0.2013,  0.1451,  0.3288,\n",
      "        -0.2342,  0.1900, -0.3094,  0.0552,  0.1467,  0.1568, -0.0589,  0.0434,\n",
      "        -0.0164, -0.1813,  0.2778,  0.0577,  0.4107,  0.1796,  0.1650,  0.0944,\n",
      "         0.2062,  0.0191,  0.1423, -0.1920,  0.1584, -0.0283, -0.2278,  0.1716,\n",
      "         0.2226, -0.3850,  0.2120, -0.0893,  0.0975,  0.0574,  0.1797, -0.0531,\n",
      "         0.1657,  0.1586, -0.1377, -0.0862, -0.2288,  0.0093,  0.1550, -0.2808,\n",
      "         0.0395, -0.0422, -0.3487,  0.1115,  0.3588, -0.1024, -0.2556, -0.1046,\n",
      "        -0.0550,  0.2014, -0.1703, -0.1451, -0.1597,  0.1791, -0.0732,  0.1887,\n",
      "        -0.1439, -0.0680, -0.1982, -0.0514,  0.1643, -0.1722,  0.3558, -0.2746,\n",
      "         0.0178, -0.0696,  0.1514,  0.0051, -0.1075,  0.2868,  0.2435, -0.0712,\n",
      "        -0.1358,  0.0575, -0.2580,  0.0681,  0.0336,  0.2508,  0.0120, -0.1143,\n",
      "        -0.3433, -0.1314, -0.0814, -0.1530, -0.0136,  0.1638,  0.0722, -0.0525,\n",
      "         0.0922, -0.1415,  0.0696, -0.0053,  0.0735, -0.0290, -0.0091,  0.0137,\n",
      "         0.1016,  0.0332,  0.0913,  0.2284, -0.1218,  0.1337,  0.0408,  0.1641,\n",
      "         0.0403, -0.1999, -0.1341, -0.0353,  0.1089, -0.0103, -0.2788,  0.1375,\n",
      "        -0.1119, -0.2747,  0.1489, -0.0939, -0.3531, -0.1668,  0.0218, -0.0101,\n",
      "         0.1003,  0.1466, -0.2995, -0.1465, -0.0823, -0.0705,  0.2328,  0.0445,\n",
      "        -0.0674, -0.0351, -0.0623, -0.2861, -0.0832, -0.0515, -0.0573,  0.0873,\n",
      "         0.0507, -0.1236, -0.3805, -0.2244,  0.2233,  0.0679, -0.1264,  0.0944,\n",
      "         0.0214,  0.1334,  0.1547,  0.1218, -0.2290,  0.0728, -0.0402,  0.4046,\n",
      "         0.1725, -0.0907, -0.0097,  0.3576, -0.4107,  0.0324, -0.0747, -0.0023,\n",
      "         0.1435,  0.1721, -0.0932, -0.1678, -0.0928,  0.2077,  0.2427,  0.2855,\n",
      "        -0.1789,  0.2511, -0.0042,  0.0768, -0.0184, -0.0390, -0.0726, -0.1978,\n",
      "        -0.0185, -0.2237, -0.0864,  0.0300, -0.0753, -0.0622, -0.0744, -0.0704,\n",
      "         0.2016, -0.0112, -0.1600,  0.0243,  0.0952, -0.2523, -0.0840,  0.3184,\n",
      "         0.0421, -0.2965,  0.0345,  0.0919,  0.1414, -0.0612,  0.1647,  0.1734],\n",
      "       grad_fn=<AddBackward0>)\n",
      "The output size: is torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa304f8",
   "metadata": {},
   "source": [
    "# Gather output for layer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e745b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "     # Gather the output for modules for the specified layers\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    # Registering the hook for the layer in desired_layer\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "\n",
    "# Define layers to capture\n",
    "desired_layers = ['layer4']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "\n",
    "#Sliding Window Technique with window_size and step\n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    #loop through every window of size 32\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach  \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        cv2.imshow(\"The captured frame is: \", frame)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #Defining the maximum frames to 32 and step as 16\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    # Get sequences using sliding window\n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    \n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        # Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    # Stack outputs along a new dimension\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    print(\"The output is: \", output_current_layer.shape)\n",
    "    print(\"The output size: is\",output_current_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d576d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is:  torch.Size([512])\n",
      "The output size: is tensor([7.7579e-01, 4.5129e-01, 2.0717e-01, 1.6730e+00, 8.9261e-01, 3.6058e-01,\n",
      "        2.4383e-01, 5.6395e-02, 1.7158e-01, 2.5123e-01, 3.4304e-01, 1.3392e+00,\n",
      "        2.9629e-01, 2.7374e-01, 3.4943e-01, 3.2740e-01, 2.7913e-01, 9.6901e-01,\n",
      "        7.1079e-01, 5.3321e-01, 3.8604e-01, 3.0036e-01, 5.4745e-01, 2.0259e-01,\n",
      "        7.1918e-01, 7.5260e-01, 4.9647e-02, 2.1695e-01, 6.9807e-01, 1.0871e+00,\n",
      "        4.1526e-01, 2.7820e-02, 5.7344e-01, 4.3983e-01, 5.9127e-01, 3.0679e-01,\n",
      "        5.5573e-01, 7.2641e-01, 1.8070e-02, 1.5918e+00, 1.5167e-01, 1.2040e-01,\n",
      "        4.5441e-01, 6.0110e-01, 1.4058e+00, 7.6430e-02, 2.6468e-01, 4.4147e-02,\n",
      "        3.6267e-01, 1.6038e+00, 1.4299e+00, 6.2218e-01, 4.0504e-01, 3.3926e-01,\n",
      "        8.3149e-01, 4.0040e-01, 9.8210e-01, 4.4458e-01, 3.9537e-03, 5.5132e-01,\n",
      "        1.5499e-01, 5.2958e-01, 5.2714e-01, 1.9877e+00, 1.6646e+00, 4.6035e-01,\n",
      "        2.7412e-01, 4.4741e-01, 3.3356e-01, 2.5586e-01, 2.7008e-01, 8.6741e-02,\n",
      "        1.1212e+00, 3.7313e-01, 1.1063e+00, 1.2933e+00, 2.7429e-01, 1.1065e+00,\n",
      "        5.5682e-02, 3.2388e-01, 4.7872e-01, 2.1672e-01, 1.1095e+00, 1.2546e-01,\n",
      "        9.6020e-02, 2.0004e-01, 1.1350e+00, 3.4431e-01, 8.2368e-01, 9.0886e-01,\n",
      "        5.2987e-01, 3.0504e-01, 1.0005e+00, 4.5637e-02, 2.8291e-01, 6.2563e-01,\n",
      "        4.4022e-01, 1.0730e+00, 1.6659e+00, 9.2557e-01, 8.1421e-02, 1.0663e-01,\n",
      "        3.1884e-01, 7.8340e-01, 6.8383e-01, 1.4139e+00, 9.9572e-02, 1.1090e+00,\n",
      "        2.4099e-01, 1.3481e+00, 7.8609e-01, 8.0484e-01, 1.0826e-01, 5.5491e-01,\n",
      "        2.1479e+00, 1.0002e+00, 1.0818e+00, 2.8685e-01, 4.7461e-01, 1.2653e+00,\n",
      "        5.4496e-01, 8.6571e-01, 3.5552e-01, 1.6212e-01, 3.3794e-01, 1.8008e-01,\n",
      "        1.0038e+00, 9.1076e-01, 1.1255e+00, 6.0586e-01, 1.7843e+00, 1.3068e-01,\n",
      "        1.3707e-01, 5.5866e-01, 1.2432e+00, 9.8958e-02, 1.0917e-01, 8.5538e-01,\n",
      "        4.5276e-01, 1.1524e-01, 3.9429e-01, 1.9213e+00, 7.0417e-01, 4.7531e-01,\n",
      "        5.5028e-01, 9.5131e-02, 1.1089e-01, 1.5987e+00, 4.6766e-01, 2.2563e-01,\n",
      "        3.6430e-01, 1.5699e-03, 1.1988e+00, 3.5379e-01, 9.9773e-01, 1.0722e+00,\n",
      "        5.2926e-01, 9.9773e-01, 1.1776e+00, 1.2767e+00, 1.5126e-01, 1.7797e-01,\n",
      "        3.2327e-02, 2.1887e-01, 1.2310e+00, 5.6731e-02, 1.6536e-01, 1.0674e+00,\n",
      "        5.1567e-01, 3.6400e-01, 1.2627e-01, 4.6602e-01, 9.6730e-02, 5.1780e-01,\n",
      "        2.3105e-01, 5.2324e-01, 1.0209e-01, 1.5685e-01, 3.4982e-01, 7.4229e-02,\n",
      "        3.9553e-01, 1.8838e+00, 1.5036e-01, 2.9451e-01, 8.8558e-01, 1.3151e-01,\n",
      "        7.4562e-01, 1.9816e-01, 1.8947e-01, 3.0575e-01, 5.8798e-01, 9.9262e-01,\n",
      "        1.1857e+00, 3.6559e-02, 1.5228e+00, 7.9705e-01, 1.5937e-01, 6.9900e-01,\n",
      "        1.7863e+00, 4.5153e-01, 8.6214e-01, 3.1005e-01, 4.0809e-01, 6.0079e-01,\n",
      "        9.0027e-01, 1.6358e-01, 2.3190e+00, 1.2242e-01, 7.0262e-01, 1.4171e-01,\n",
      "        1.8705e+00, 1.1118e+00, 4.8448e-01, 2.2268e+00, 8.8349e-01, 4.0341e-01,\n",
      "        2.6148e-01, 1.2424e+00, 7.5073e-02, 8.1088e-02, 1.3519e+00, 9.1690e-01,\n",
      "        3.9431e-02, 3.7708e-01, 1.7585e+00, 4.4842e-01, 3.9821e-01, 3.6077e-01,\n",
      "        9.7705e-01, 6.9266e-01, 6.4223e-01, 1.7649e+00, 1.0529e-01, 9.5751e-01,\n",
      "        5.6519e-01, 1.4064e-01, 2.9176e-01, 7.0234e-01, 2.2236e-01, 3.8264e-02,\n",
      "        9.1057e-01, 6.1459e-01, 3.9115e-01, 1.0555e-01, 1.1037e+00, 4.4490e-01,\n",
      "        5.6297e-01, 1.1013e+00, 7.0639e-01, 4.0298e-02, 6.7317e-01, 9.1655e-01,\n",
      "        5.9270e-01, 7.7877e-01, 5.1742e-02, 1.3579e-01, 1.3163e+00, 4.1447e-01,\n",
      "        6.3269e-01, 7.0438e-02, 1.3338e+00, 1.8937e-01, 3.2501e-01, 5.8101e-01,\n",
      "        1.3411e-01, 1.6261e-01, 7.4628e-01, 9.9760e-01, 9.4337e-02, 6.7948e-01,\n",
      "        1.8297e-01, 5.4644e-01, 7.8763e-01, 1.1144e+00, 2.6790e-01, 8.8858e-01,\n",
      "        2.0045e+00, 7.6746e-01, 6.8928e-01, 2.8957e-01, 5.1223e-01, 3.2734e-01,\n",
      "        5.4822e-01, 2.0978e+00, 3.3893e-01, 1.4505e-01, 1.0428e-01, 1.8292e-01,\n",
      "        1.0493e+00, 2.6773e-01, 2.5462e-01, 7.5169e-01, 1.5631e-01, 7.5948e-01,\n",
      "        1.1135e+00, 5.6013e-01, 2.6218e-01, 6.3303e-01, 5.7903e-01, 7.0778e-01,\n",
      "        1.8023e-01, 1.0111e+00, 2.4711e-01, 2.6107e-01, 9.1166e-01, 9.9638e-01,\n",
      "        2.8284e-01, 2.2672e-01, 3.2048e-01, 6.8218e-01, 2.5453e-01, 7.4655e-01,\n",
      "        1.7998e-01, 1.1030e+00, 1.3836e+00, 6.2955e-01, 6.2815e-02, 2.2085e-01,\n",
      "        9.9471e-01, 1.3865e-01, 1.8148e+00, 7.2738e-01, 8.7992e-01, 5.8968e-01,\n",
      "        5.8652e-01, 2.2547e-02, 2.3347e+00, 1.0705e-01, 3.1678e+00, 8.8123e-03,\n",
      "        1.3105e+00, 3.6242e-02, 8.6724e-01, 1.3707e+00, 8.7496e-01, 1.1584e+00,\n",
      "        1.3434e-01, 1.2488e+00, 3.8676e-01, 7.5586e-01, 1.3605e+00, 7.1131e-01,\n",
      "        8.5230e-01, 5.0145e-01, 7.7843e-01, 5.6135e-01, 6.6616e-01, 1.8539e+00,\n",
      "        1.7722e-01, 1.0690e+00, 6.0987e-01, 1.1963e+00, 6.0859e-01, 2.6147e-01,\n",
      "        1.3850e+00, 7.9732e-01, 4.7997e-01, 1.9481e+00, 9.2436e-01, 6.8382e-02,\n",
      "        1.9602e+00, 5.7410e-01, 7.7569e-01, 3.6824e-01, 9.4233e-02, 1.1855e+00,\n",
      "        4.5004e-01, 3.8699e-01, 5.5451e-01, 1.2740e+00, 6.0985e-01, 6.6221e-01,\n",
      "        7.8207e-02, 3.7348e-01, 4.2298e-01, 2.6192e-01, 2.5512e-01, 3.9916e-01,\n",
      "        6.8960e-01, 2.2682e-02, 1.0879e+00, 8.7622e-02, 6.0176e-01, 7.5113e-01,\n",
      "        9.0410e-02, 2.2768e-01, 6.1994e-01, 1.9141e-01, 4.5728e-01, 5.3628e-01,\n",
      "        5.1149e-01, 8.5386e-02, 2.5664e-01, 4.1782e-01, 4.0646e-01, 2.9412e-01,\n",
      "        2.6603e-01, 3.8213e-02, 8.7337e-01, 4.7538e-01, 2.7382e-01, 4.2270e-01,\n",
      "        4.8860e-01, 1.1714e+00, 5.9338e-02, 2.5214e-01, 1.2117e-01, 9.3003e-01,\n",
      "        8.2440e-01, 3.7790e-01, 1.1441e+00, 1.4989e+00, 5.3185e-01, 1.4816e+00,\n",
      "        3.4659e-01, 2.7584e-01, 1.4356e-01, 1.4467e+00, 4.5015e-01, 5.4210e-01,\n",
      "        1.4965e-01, 1.8073e+00, 2.4736e-01, 9.3415e-01, 2.2440e-01, 8.5739e-01,\n",
      "        1.1091e-01, 3.2383e-01, 6.6050e-02, 6.8104e-02, 9.7989e-01, 5.3433e-01,\n",
      "        7.7279e-01, 1.5967e-01, 1.1298e+00, 4.0054e-01, 6.9831e-01, 7.7493e-01,\n",
      "        9.5973e-02, 6.3851e-01, 1.4386e+00, 1.1566e+00, 3.8189e-01, 4.9318e-01,\n",
      "        9.5232e-02, 1.4426e-01, 2.5910e-02, 3.0549e-02, 5.7549e-01, 6.2886e-01,\n",
      "        2.9116e-01, 1.5590e+00, 3.6730e-01, 3.5595e-01, 2.4795e-01, 3.5964e-01,\n",
      "        2.3540e+00, 3.4370e-01, 5.1058e-01, 9.2361e-02, 2.1978e-01, 7.6361e-01,\n",
      "        8.7558e-01, 3.9460e-01, 6.9728e-01, 2.5736e-01, 2.1905e+00, 7.5499e-01,\n",
      "        1.1618e-01, 2.2154e-01, 3.0171e-01, 2.8600e-01, 3.1989e-01, 1.2603e-01,\n",
      "        4.2908e-01, 1.2696e+00, 1.4848e+00, 5.8165e-01, 2.3164e-01, 3.2020e-01,\n",
      "        1.2508e+00, 1.0756e+00, 6.7063e-01, 8.3704e-01, 4.8116e-01, 7.0230e-01,\n",
      "        1.0621e-01, 8.2496e-01, 5.5748e-01, 7.2114e-01, 1.2964e-01, 4.4469e-01,\n",
      "        5.8240e-01, 8.3735e-01, 2.4377e-01, 6.1536e-01, 7.4485e-01, 1.2401e-01,\n",
      "        2.1167e+00, 1.3404e-01, 5.9433e-01, 3.2866e-01, 9.0177e-02, 8.3866e-01,\n",
      "        1.4433e+00, 1.1024e+00, 5.9231e-01, 1.2122e-01, 5.0731e-01, 1.1744e-01,\n",
      "        1.9564e-01, 4.8017e-01])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02712e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is:  torch.Size([512])\n",
      "The output size: is tensor([1.6837, 0.6771, 0.7004, 0.6793, 1.2531, 0.3773, 0.3125, 0.4773, 0.6487,\n",
      "        0.5478, 1.4792, 1.7842, 0.6825, 0.9925, 1.1459, 1.1041, 1.0127, 2.1688,\n",
      "        1.7884, 0.6928, 1.1464, 0.4939, 0.7932, 0.9736, 1.0556, 1.1016, 0.8514,\n",
      "        1.0055, 0.9254, 1.1441, 0.5119, 1.1958, 0.5374, 1.2089, 1.1962, 0.8480,\n",
      "        0.4494, 1.3445, 0.4514, 2.0262, 0.4957, 0.9180, 0.4583, 1.4063, 2.3397,\n",
      "        1.0352, 1.2072, 0.3386, 0.4911, 1.7410, 2.0319, 1.0079, 1.8192, 1.0472,\n",
      "        0.9110, 1.2798, 3.0166, 0.6431, 0.2193, 0.9200, 0.9569, 0.8908, 0.5870,\n",
      "        0.9834, 3.0343, 1.9360, 1.7962, 1.4064, 1.7433, 1.2147, 0.8337, 1.1533,\n",
      "        0.7220, 1.8619, 1.0576, 2.4818, 0.9272, 0.4529, 0.2872, 0.9707, 0.6102,\n",
      "        0.9694, 1.2390, 0.7037, 0.3970, 0.6868, 1.5592, 0.9836, 0.8231, 1.7682,\n",
      "        1.7261, 1.6992, 1.4541, 0.3497, 1.8535, 1.2403, 0.4826, 1.3325, 1.3015,\n",
      "        0.7488, 2.2028, 0.5315, 0.3048, 2.6131, 0.8301, 1.7838, 1.7340, 4.0912,\n",
      "        1.4611, 1.5663, 1.2041, 1.6874, 0.5721, 2.3241, 2.4020, 1.0616, 0.5560,\n",
      "        0.5855, 2.0766, 0.7740, 0.8104, 1.3330, 1.0231, 0.3076, 0.6863, 1.0203,\n",
      "        1.9567, 0.7061, 2.2958, 1.1148, 1.9220, 1.1826, 0.4598, 1.5304, 0.4656,\n",
      "        0.4476, 0.5612, 1.4359, 0.8095, 0.6532, 0.9877, 2.2032, 0.3178, 1.6533,\n",
      "        1.0402, 0.1679, 0.6610, 2.5243, 1.2367, 0.4461, 0.4579, 0.5555, 1.3119,\n",
      "        1.6402, 2.3210, 1.5454, 2.0428, 1.3870, 0.8649, 0.8763, 0.3565, 0.6718,\n",
      "        1.1495, 0.7672, 4.2413, 0.6504, 0.5522, 1.3048, 1.9814, 0.7276, 1.0005,\n",
      "        1.5771, 1.1229, 0.8191, 0.8470, 1.1178, 1.0122, 1.2023, 0.7943, 0.7896,\n",
      "        0.7579, 3.2429, 0.2914, 1.8292, 0.6914, 1.1157, 2.2704, 1.4214, 0.8611,\n",
      "        0.5097, 0.7832, 1.3108, 1.7647, 0.9395, 1.2355, 1.7703, 0.8689, 1.5815,\n",
      "        2.9672, 1.0078, 1.3434, 0.8381, 1.2607, 0.7907, 0.8131, 0.6501, 0.9372,\n",
      "        1.0709, 0.7026, 1.0952, 1.5470, 1.1041, 0.3765, 1.6365, 0.7954, 0.7703,\n",
      "        0.4519, 1.4104, 0.8062, 0.1468, 0.6916, 0.3756, 0.6944, 2.1370, 1.3745,\n",
      "        0.7642, 1.0534, 0.4457, 1.5763, 1.0589, 1.5905, 1.9972, 0.7776, 1.2541,\n",
      "        0.8903, 1.2310, 0.7768, 0.7176, 2.5008, 0.6303, 1.0057, 2.0514, 1.5551,\n",
      "        0.3529, 2.3909, 1.4178, 1.3310, 1.1041, 1.9554, 1.5680, 2.3553, 1.0070,\n",
      "        1.1569, 1.2398, 0.3405, 0.4922, 1.0796, 0.7254, 1.3400, 0.3914, 1.6712,\n",
      "        0.7785, 1.0187, 0.7243, 0.8189, 0.9677, 0.9751, 1.7530, 0.6565, 1.8681,\n",
      "        1.4404, 3.0373, 1.5826, 1.8791, 0.8423, 0.5952, 1.5328, 1.0231, 1.3706,\n",
      "        1.3836, 1.9136, 0.7282, 0.4087, 2.4609, 1.5012, 1.8789, 0.4931, 0.2668,\n",
      "        1.1196, 0.0763, 0.9868, 0.9889, 0.6284, 1.2967, 3.1560, 0.5203, 0.5332,\n",
      "        0.5594, 0.7367, 0.9911, 0.6227, 1.3547, 1.2667, 0.0242, 3.1033, 0.5969,\n",
      "        1.5809, 0.5679, 1.7664, 1.1397, 0.4330, 1.6730, 0.8324, 1.6034, 2.2840,\n",
      "        1.4765, 0.3960, 1.0334, 1.7088, 0.4996, 2.2831, 1.1255, 0.5558, 2.2340,\n",
      "        0.7429, 0.0145, 3.0675, 0.1620, 3.0688, 0.8691, 1.0177, 0.5769, 0.9939,\n",
      "        1.4139, 1.7686, 0.9650, 1.5772, 2.9411, 0.7290, 1.1109, 3.1600, 2.1901,\n",
      "        1.2807, 1.5999, 1.5193, 1.1002, 2.2421, 1.8479, 0.4959, 1.2461, 2.3137,\n",
      "        1.8618, 1.2105, 0.6324, 0.8795, 1.2126, 0.3530, 1.1329, 1.1883, 0.7961,\n",
      "        2.9684, 0.6094, 1.4679, 0.8719, 1.7528, 2.5115, 0.4899, 1.0853, 1.8012,\n",
      "        2.2313, 2.1199, 1.3887, 0.1715, 1.3594, 0.8005, 0.8480, 1.4917, 0.9010,\n",
      "        1.3156, 0.8999, 1.6725, 1.3689, 2.1222, 1.7920, 0.6053, 1.6780, 1.3039,\n",
      "        0.9785, 2.6147, 1.8607, 0.3743, 0.4024, 0.7318, 1.3012, 1.3015, 0.1215,\n",
      "        1.2465, 0.2640, 0.9805, 1.7875, 0.7397, 0.7585, 0.1222, 1.9923, 0.4504,\n",
      "        1.0547, 0.8180, 1.4866, 0.6533, 0.9321, 0.7254, 1.5262, 0.4704, 1.3756,\n",
      "        0.5070, 0.4676, 1.2156, 1.1267, 1.0738, 0.5522, 1.1011, 1.7054, 0.7306,\n",
      "        1.8368, 0.4068, 2.6230, 0.6539, 0.4532, 0.6588, 1.1696, 2.2771, 2.0192,\n",
      "        2.3551, 0.5495, 2.6279, 1.2110, 1.0054, 1.2948, 0.3586, 1.5414, 1.3431,\n",
      "        2.0253, 0.6156, 2.1443, 0.7380, 0.5740, 0.0595, 0.6190, 1.4912, 1.1349,\n",
      "        0.4151, 2.9679, 0.8530, 0.3955, 0.6645, 1.5990, 1.7387, 0.3713, 1.1080,\n",
      "        0.7447, 0.4070, 1.5943, 0.7535, 0.9172, 1.4381, 1.4266, 1.6030, 0.6645,\n",
      "        0.6083, 0.5508, 1.1509, 0.8707, 0.3153, 1.1178, 1.2716, 2.9192, 2.5396,\n",
      "        0.8830, 0.6922, 1.4285, 3.0278, 0.9172, 0.7383, 2.1808, 2.1068, 0.7795,\n",
      "        1.4852, 3.1249, 1.2719, 0.6978, 0.3857, 2.0453, 0.0900, 0.3528, 0.3664,\n",
      "        1.6640, 0.9971, 0.8822, 1.8845, 0.7272, 0.8961, 0.6112, 1.0499, 1.2176,\n",
      "        2.3090, 1.9807, 0.9410, 1.0534, 1.0364, 1.1323, 0.4632, 0.3392])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09d7f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is:  torch.Size([512])\n",
      "The output size: is tensor([0.9367, 0.4827, 1.0497, 0.4203, 0.3126, 0.4230, 1.2803, 0.3168, 0.6442,\n",
      "        0.9106, 0.7423, 2.5590, 1.0426, 1.0067, 0.9097, 0.2099, 0.2941, 0.4455,\n",
      "        0.3833, 0.5824, 0.0207, 0.4135, 0.8301, 0.8446, 0.5102, 1.0307, 0.7937,\n",
      "        0.3483, 0.9858, 0.6824, 0.1545, 0.6911, 0.7652, 1.0923, 1.2133, 1.5107,\n",
      "        0.3752, 2.1716, 0.9251, 0.7581, 0.3904, 0.1953, 0.2319, 1.0024, 1.1077,\n",
      "        0.4836, 0.6344, 0.9908, 0.7618, 2.8145, 0.5304, 1.2291, 1.4844, 1.0228,\n",
      "        0.2132, 0.5384, 0.4033, 0.5564, 0.7473, 0.8041, 0.1867, 1.1768, 0.1432,\n",
      "        0.5795, 0.7811, 0.0133, 0.7557, 1.0523, 1.9415, 0.7934, 0.2676, 1.0487,\n",
      "        1.9105, 2.5016, 1.7352, 0.1147, 0.1244, 0.2670, 0.1145, 0.2925, 0.3337,\n",
      "        0.9030, 0.9493, 1.3562, 0.2458, 1.0983, 1.0096, 1.2707, 0.8021, 1.8631,\n",
      "        0.7592, 0.5140, 0.3852, 0.8677, 0.7106, 1.4081, 0.3101, 0.3991, 0.9322,\n",
      "        1.0870, 0.2850, 0.5233, 0.3938, 2.3760, 1.3855, 2.0014, 0.7741, 1.7549,\n",
      "        0.3006, 0.7468, 0.0223, 0.9510, 0.4350, 1.2673, 0.9652, 1.6581, 0.8418,\n",
      "        0.1723, 1.1296, 1.3268, 0.0872, 0.5253, 0.6316, 1.4987, 0.4356, 0.5121,\n",
      "        0.2422, 0.0507, 0.4875, 0.7172, 0.9971, 0.1613, 0.5199, 1.8544, 1.1021,\n",
      "        1.4755, 0.8438, 0.2733, 0.7590, 0.0855, 0.7833, 1.0038, 0.6156, 0.7364,\n",
      "        0.7955, 0.1036, 0.7329, 1.9173, 0.8855, 0.0631, 0.4627, 0.2047, 0.9929,\n",
      "        0.5724, 0.3505, 0.6554, 0.4305, 0.1074, 0.5443, 0.5309, 0.6393, 0.4395,\n",
      "        0.1313, 0.1408, 1.2759, 0.2719, 0.5835, 0.4592, 0.1493, 0.6984, 0.8210,\n",
      "        1.1920, 0.6473, 0.4599, 0.0765, 0.8444, 0.5507, 0.3346, 1.7194, 0.0051,\n",
      "        0.7227, 1.7318, 0.3325, 0.8800, 0.7697, 0.0583, 0.6320, 0.2417, 0.1330,\n",
      "        0.0900, 0.5023, 1.1556, 0.7945, 0.6777, 0.9416, 0.9314, 0.2949, 0.9925,\n",
      "        2.9900, 1.1589, 0.8009, 0.3715, 1.3614, 0.1129, 1.9141, 0.8194, 0.9116,\n",
      "        0.5159, 0.8065, 0.5745, 0.9489, 0.3015, 0.4651, 3.1661, 1.0365, 0.1852,\n",
      "        0.0803, 0.9185, 0.3466, 0.7288, 1.8984, 1.6442, 0.5026, 1.3802, 1.7508,\n",
      "        0.8012, 0.4661, 0.1023, 0.2429, 1.1034, 0.7574, 1.5004, 0.3619, 0.2715,\n",
      "        0.9359, 0.4095, 0.3095, 2.0019, 1.5818, 0.0845, 2.3352, 1.1458, 1.4300,\n",
      "        2.2718, 0.1260, 0.3796, 0.6724, 0.6027, 0.1812, 0.2443, 0.3244, 1.1935,\n",
      "        0.4170, 0.5230, 0.5255, 0.3075, 0.3140, 0.1023, 0.1722, 0.7085, 0.5723,\n",
      "        0.9361, 0.0092, 0.2569, 1.9874, 0.1801, 1.1171, 0.5139, 1.6033, 0.6138,\n",
      "        0.8880, 0.8311, 0.5899, 1.0455, 0.1167, 0.7550, 1.8487, 1.0486, 0.1512,\n",
      "        0.3645, 2.5716, 1.1689, 0.2481, 1.2156, 2.4775, 1.3333, 0.4828, 0.6415,\n",
      "        1.0060, 0.1158, 0.2508, 0.2384, 0.8595, 0.3581, 0.3661, 0.8032, 0.0597,\n",
      "        1.6066, 1.5509, 1.1581, 0.5037, 0.6888, 0.7432, 0.1417, 0.7572, 0.4204,\n",
      "        1.3441, 0.5536, 1.5926, 1.3141, 1.1197, 0.6890, 0.2004, 0.5556, 2.0727,\n",
      "        0.4399, 0.2553, 1.1788, 1.2856, 0.2093, 3.6023, 2.0037, 0.5838, 2.5186,\n",
      "        1.2824, 0.4956, 1.8900, 0.7222, 1.3565, 0.8000, 0.5538, 0.2677, 0.9881,\n",
      "        0.7137, 0.9791, 0.4760, 1.4209, 1.1065, 1.0632, 0.7790, 1.7809, 0.0943,\n",
      "        1.8932, 1.7933, 0.4632, 0.9899, 1.8422, 0.4624, 0.6659, 0.5537, 0.9030,\n",
      "        1.4047, 0.9525, 0.2982, 0.6683, 1.1628, 0.6319, 0.8285, 1.3447, 0.1723,\n",
      "        1.5444, 0.2768, 1.3761, 0.5259, 1.1226, 1.0510, 0.2649, 0.2152, 0.7347,\n",
      "        1.1471, 0.6070, 0.8866, 0.2892, 0.4140, 0.4914, 0.7607, 0.1996, 0.0187,\n",
      "        1.3890, 0.4726, 0.7826, 0.4491, 1.2489, 2.0784, 0.4796, 0.1789, 0.8979,\n",
      "        0.4368, 1.3226, 0.9622, 0.4105, 0.4860, 0.6464, 0.7005, 0.9958, 0.4201,\n",
      "        0.9721, 0.0504, 1.5893, 1.6183, 0.2742, 1.2421, 0.4645, 1.5099, 0.0921,\n",
      "        0.3902, 0.0316, 0.9919, 0.2305, 0.2114, 0.8346, 0.2746, 0.6592, 0.9353,\n",
      "        0.2139, 1.1212, 1.1753, 1.1456, 0.1404, 0.2599, 0.5011, 0.6463, 1.1406,\n",
      "        1.0827, 0.1708, 1.4834, 1.1235, 0.0621, 0.3979, 1.8255, 1.4775, 1.1972,\n",
      "        1.2829, 0.1642, 0.9397, 0.1591, 0.2751, 0.4842, 0.4585, 1.8931, 0.7967,\n",
      "        1.7550, 0.7022, 1.8260, 0.2711, 0.3447, 0.3948, 0.3484, 0.0336, 1.5022,\n",
      "        0.2620, 2.6037, 2.0490, 1.0803, 0.9049, 0.3076, 1.5694, 0.9732, 0.6007,\n",
      "        1.0895, 0.0415, 0.6835, 0.8585, 0.1799, 0.2409, 0.9185, 1.9297, 0.1107,\n",
      "        0.3751, 0.0135, 0.3846, 0.4276, 0.6155, 0.4991, 0.7388, 0.8524, 0.4091,\n",
      "        1.0062, 0.9677, 1.6288, 1.9786, 0.5334, 1.1182, 1.2415, 0.5000, 0.1940,\n",
      "        1.6368, 1.6449, 2.2013, 1.0446, 0.8019, 0.7068, 1.4805, 0.3624, 0.4599,\n",
      "        0.3416, 0.4430, 0.1325, 1.0248, 0.3281, 0.3701, 0.2818, 0.2907, 0.7026,\n",
      "        1.2272, 0.5634, 0.0540, 0.1594, 0.3825, 0.6647, 0.6317, 1.3497])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee85b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is:  torch.Size([512])\n",
      "The output size: is tensor([2.4411, 0.5666, 1.8598, 2.0171, 1.9870, 1.8907, 2.0599, 1.3425, 0.8550,\n",
      "        2.7212, 1.6989, 1.2895, 1.3887, 1.9664, 0.9090, 1.5021, 1.9278, 1.6439,\n",
      "        2.8154, 1.6013, 1.0401, 1.3525, 1.4834, 2.5557, 1.0083, 1.6117, 0.9043,\n",
      "        1.1451, 1.3762, 2.2180, 0.3626, 0.6087, 2.4042, 1.2277, 1.4794, 1.1843,\n",
      "        1.3084, 1.0991, 0.7163, 2.8924, 1.4175, 1.5645, 0.6755, 2.5139, 2.5528,\n",
      "        1.3097, 0.7739, 0.3060, 1.3931, 1.0105, 2.1934, 3.1912, 1.5876, 1.7339,\n",
      "        2.1120, 2.0152, 2.5356, 0.9265, 1.4910, 2.1628, 1.0012, 1.5088, 0.7987,\n",
      "        2.6206, 1.2426, 1.9988, 1.3777, 2.1804, 2.5331, 1.6962, 0.7735, 1.3231,\n",
      "        2.1047, 0.8420, 1.2953, 1.6759, 1.2353, 1.3414, 1.4294, 1.3430, 2.3848,\n",
      "        3.3447, 2.1617, 1.1585, 1.7472, 0.7377, 2.9514, 1.0049, 1.8954, 1.7356,\n",
      "        1.7746, 1.4275, 0.8666, 1.1147, 1.4269, 0.8324, 2.3343, 2.5240, 1.4658,\n",
      "        1.6645, 0.1974, 2.3741, 0.9591, 1.3320, 0.4813, 1.1087, 2.4694, 2.1430,\n",
      "        1.2756, 1.9526, 2.7647, 2.0399, 0.5097, 1.8200, 0.7931, 2.1257, 1.5963,\n",
      "        0.2693, 2.2735, 2.4426, 0.6545, 0.9401, 0.6845, 1.2698, 2.5833, 2.0959,\n",
      "        1.8751, 2.6789, 2.3257, 0.9992, 3.2992, 1.8620, 0.9896, 2.8791, 2.6921,\n",
      "        0.7663, 1.2770, 1.1772, 0.8603, 0.4755, 0.8982, 2.5694, 2.1872, 1.3532,\n",
      "        1.5398, 2.4160, 0.7351, 1.7352, 1.9692, 2.2666, 1.4788, 2.0681, 1.0850,\n",
      "        1.3138, 0.8737, 1.7972, 1.4094, 1.7485, 1.8310, 3.2812, 0.5000, 2.4108,\n",
      "        1.1914, 1.2781, 3.1271, 1.1957, 1.2744, 2.9437, 0.9778, 3.0768, 0.9569,\n",
      "        1.4112, 0.4492, 2.0255, 0.1755, 0.1755, 2.2863, 0.9054, 0.4272, 1.1868,\n",
      "        0.5871, 3.2738, 1.0148, 2.4109, 0.1710, 2.3231, 1.5403, 1.2706, 1.5954,\n",
      "        0.9632, 0.6489, 2.5944, 0.9349, 1.5493, 1.3273, 2.4721, 1.0637, 0.7135,\n",
      "        1.5324, 1.5223, 2.9374, 2.2887, 1.4837, 1.5214, 3.4297, 0.5826, 1.1749,\n",
      "        2.4416, 1.1749, 0.8395, 3.0369, 2.4218, 1.8439, 2.3373, 1.7185, 2.2428,\n",
      "        2.3035, 0.9879, 1.0359, 0.7096, 1.2655, 0.9996, 0.8523, 2.9838, 2.5968,\n",
      "        1.3613, 3.0340, 1.0452, 2.9877, 3.4354, 1.5936, 1.5841, 1.4783, 0.8415,\n",
      "        1.2617, 2.0439, 1.1934, 2.4773, 2.6000, 2.0643, 2.0489, 3.2353, 1.8378,\n",
      "        1.0358, 2.4351, 1.9278, 1.6423, 1.2592, 1.6289, 3.4013, 1.8147, 2.6140,\n",
      "        1.1031, 1.7232, 1.4461, 1.6724, 3.2491, 0.7747, 1.1967, 1.1093, 3.5324,\n",
      "        2.0729, 0.3906, 1.8579, 0.6692, 2.7174, 1.0901, 2.2886, 2.2626, 2.2127,\n",
      "        0.5721, 2.8210, 2.6976, 1.7612, 2.4468, 1.3251, 1.6815, 2.1122, 1.4666,\n",
      "        1.8540, 2.8717, 2.1729, 1.4109, 1.6088, 2.3218, 1.0791, 0.9756, 0.5519,\n",
      "        0.6457, 1.7071, 2.3910, 0.9626, 1.5928, 2.1640, 3.9242, 1.6856, 1.6719,\n",
      "        1.6913, 2.1174, 2.6137, 1.5733, 1.9606, 3.0058, 0.5126, 0.9495, 2.4239,\n",
      "        1.1437, 2.1158, 1.9105, 1.9359, 1.4967, 3.0482, 2.9136, 1.3807, 3.1278,\n",
      "        1.1904, 0.2268, 0.8436, 1.4301, 0.7976, 2.5018, 2.8828, 1.1701, 3.7926,\n",
      "        1.4397, 1.2131, 1.3083, 1.5277, 3.2438, 1.7489, 0.8465, 1.5585, 1.7251,\n",
      "        1.7447, 2.5846, 1.3451, 0.9821, 3.4911, 1.4556, 1.2324, 2.0555, 1.6240,\n",
      "        0.4609, 1.7027, 1.6731, 1.0958, 3.1049, 1.0705, 0.4049, 1.4904, 3.3322,\n",
      "        2.3631, 1.9101, 0.5585, 0.3338, 1.8762, 2.3717, 1.1169, 2.4686, 1.4388,\n",
      "        0.4792, 1.4839, 1.0757, 0.3928, 1.5007, 1.9007, 1.1969, 1.7450, 2.7849,\n",
      "        2.5893, 1.8316, 1.8586, 0.9541, 0.8998, 0.6003, 1.4617, 2.1489, 0.9052,\n",
      "        1.5438, 1.2906, 3.5800, 1.6238, 1.9456, 2.9291, 0.9544, 5.6788, 1.1469,\n",
      "        0.6782, 1.7524, 1.1053, 0.6781, 2.0331, 0.5909, 1.8729, 1.4014, 2.6419,\n",
      "        1.6194, 0.6535, 1.4931, 2.0175, 0.9587, 2.4404, 1.3178, 3.1350, 1.1663,\n",
      "        0.9881, 1.1164, 1.3462, 3.3772, 1.0020, 0.8439, 2.7168, 0.8224, 0.9848,\n",
      "        1.1797, 1.4575, 0.3516, 1.0966, 1.2176, 1.2409, 2.0848, 2.4289, 1.6431,\n",
      "        1.7237, 1.0081, 1.5712, 1.3048, 2.0533, 0.3659, 1.7207, 1.8035, 2.1144,\n",
      "        2.7119, 1.5958, 2.0403, 1.9092, 2.1786, 0.2367, 1.6017, 2.1936, 1.1524,\n",
      "        3.6356, 0.8236, 3.7795, 0.7434, 1.2105, 0.6612, 1.2753, 1.4629, 1.8923,\n",
      "        0.3431, 1.2189, 2.0909, 0.9336, 0.6363, 2.1566, 1.9975, 1.1224, 1.1572,\n",
      "        2.4953, 0.4690, 2.9200, 1.8779, 3.0539, 0.5356, 1.8627, 2.5947, 2.6399,\n",
      "        1.9062, 2.1216, 1.2586, 2.2791, 2.3691, 1.0766, 2.0730, 1.7330, 2.8503,\n",
      "        1.3333, 0.7549, 0.9773, 1.0292, 0.6349, 1.8353, 0.8615, 3.0827, 1.7382,\n",
      "        1.8525, 1.8792, 2.3640, 1.6554, 0.8084, 1.3389, 1.0359, 0.7173, 1.3860,\n",
      "        2.1632, 1.3059, 1.6968, 2.2005, 1.1620, 1.6916, 2.2634, 0.2819, 2.1457,\n",
      "        3.2276, 2.4967, 1.5160, 0.4366, 0.7722, 1.6325, 1.1110, 0.8718])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5a719",
   "metadata": {},
   "source": [
    "# Gather output for avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c29ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and sift to eval mode\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "     # Gather the output for modules for the specified layers\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    # Registering the hook for the layer in desired_layer\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "\n",
    "# Define layers to capture\n",
    "desired_layers = ['avgpool']\n",
    "layer_outputs = output_hook(desired_layers)\n",
    "\n",
    "#Sliding Window Technique with window_size and step\n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    #loop through every window of size 32\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "    \n",
    "# Function to preprocess the video and apply sliding window approach  \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        cv2.imshow(\"The captured frame is: \", frame)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #Defining the maximum frames to 32 and step as 16\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    # Get sequences using sliding window\n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skipping the video\", video_file_path)\n",
    "        return\n",
    "        #diff = 32-len(video_frames)\n",
    "        #for i in range(0, diff):\n",
    "            #video_frames.append(video_frames[i])\n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        # Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    # Stack outputs along a new dimension\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    print(\"The output is: \", output_current_layer)\n",
    "    print(\"The output size: is\",output_current_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "959316e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is:  tensor([7.7579e-01, 4.5129e-01, 2.0717e-01, 1.6730e+00, 8.9261e-01, 3.6058e-01,\n",
      "        2.4383e-01, 5.6395e-02, 1.7158e-01, 2.5123e-01, 3.4304e-01, 1.3392e+00,\n",
      "        2.9629e-01, 2.7374e-01, 3.4943e-01, 3.2740e-01, 2.7913e-01, 9.6901e-01,\n",
      "        7.1079e-01, 5.3321e-01, 3.8604e-01, 3.0036e-01, 5.4745e-01, 2.0259e-01,\n",
      "        7.1918e-01, 7.5260e-01, 4.9647e-02, 2.1695e-01, 6.9807e-01, 1.0871e+00,\n",
      "        4.1526e-01, 2.7820e-02, 5.7344e-01, 4.3983e-01, 5.9127e-01, 3.0679e-01,\n",
      "        5.5573e-01, 7.2641e-01, 1.8070e-02, 1.5918e+00, 1.5167e-01, 1.2040e-01,\n",
      "        4.5441e-01, 6.0110e-01, 1.4058e+00, 7.6430e-02, 2.6468e-01, 4.4147e-02,\n",
      "        3.6267e-01, 1.6038e+00, 1.4299e+00, 6.2218e-01, 4.0504e-01, 3.3926e-01,\n",
      "        8.3149e-01, 4.0040e-01, 9.8210e-01, 4.4458e-01, 3.9537e-03, 5.5132e-01,\n",
      "        1.5499e-01, 5.2958e-01, 5.2714e-01, 1.9877e+00, 1.6646e+00, 4.6035e-01,\n",
      "        2.7412e-01, 4.4741e-01, 3.3356e-01, 2.5586e-01, 2.7008e-01, 8.6741e-02,\n",
      "        1.1212e+00, 3.7313e-01, 1.1063e+00, 1.2933e+00, 2.7429e-01, 1.1065e+00,\n",
      "        5.5682e-02, 3.2388e-01, 4.7872e-01, 2.1672e-01, 1.1095e+00, 1.2546e-01,\n",
      "        9.6020e-02, 2.0004e-01, 1.1350e+00, 3.4431e-01, 8.2368e-01, 9.0886e-01,\n",
      "        5.2987e-01, 3.0504e-01, 1.0005e+00, 4.5637e-02, 2.8291e-01, 6.2563e-01,\n",
      "        4.4022e-01, 1.0730e+00, 1.6659e+00, 9.2557e-01, 8.1421e-02, 1.0663e-01,\n",
      "        3.1884e-01, 7.8340e-01, 6.8383e-01, 1.4139e+00, 9.9572e-02, 1.1090e+00,\n",
      "        2.4099e-01, 1.3481e+00, 7.8609e-01, 8.0484e-01, 1.0826e-01, 5.5491e-01,\n",
      "        2.1479e+00, 1.0002e+00, 1.0818e+00, 2.8685e-01, 4.7461e-01, 1.2653e+00,\n",
      "        5.4496e-01, 8.6571e-01, 3.5552e-01, 1.6212e-01, 3.3794e-01, 1.8008e-01,\n",
      "        1.0038e+00, 9.1076e-01, 1.1255e+00, 6.0586e-01, 1.7843e+00, 1.3068e-01,\n",
      "        1.3707e-01, 5.5866e-01, 1.2432e+00, 9.8958e-02, 1.0917e-01, 8.5538e-01,\n",
      "        4.5276e-01, 1.1524e-01, 3.9429e-01, 1.9213e+00, 7.0417e-01, 4.7531e-01,\n",
      "        5.5028e-01, 9.5131e-02, 1.1089e-01, 1.5987e+00, 4.6766e-01, 2.2563e-01,\n",
      "        3.6430e-01, 1.5699e-03, 1.1988e+00, 3.5379e-01, 9.9773e-01, 1.0722e+00,\n",
      "        5.2926e-01, 9.9773e-01, 1.1776e+00, 1.2767e+00, 1.5126e-01, 1.7797e-01,\n",
      "        3.2327e-02, 2.1887e-01, 1.2310e+00, 5.6731e-02, 1.6536e-01, 1.0674e+00,\n",
      "        5.1567e-01, 3.6400e-01, 1.2627e-01, 4.6602e-01, 9.6730e-02, 5.1780e-01,\n",
      "        2.3105e-01, 5.2324e-01, 1.0209e-01, 1.5685e-01, 3.4982e-01, 7.4229e-02,\n",
      "        3.9553e-01, 1.8838e+00, 1.5036e-01, 2.9451e-01, 8.8558e-01, 1.3151e-01,\n",
      "        7.4562e-01, 1.9816e-01, 1.8947e-01, 3.0575e-01, 5.8798e-01, 9.9262e-01,\n",
      "        1.1857e+00, 3.6559e-02, 1.5228e+00, 7.9705e-01, 1.5937e-01, 6.9900e-01,\n",
      "        1.7863e+00, 4.5153e-01, 8.6214e-01, 3.1005e-01, 4.0809e-01, 6.0079e-01,\n",
      "        9.0027e-01, 1.6358e-01, 2.3190e+00, 1.2242e-01, 7.0262e-01, 1.4171e-01,\n",
      "        1.8705e+00, 1.1118e+00, 4.8448e-01, 2.2268e+00, 8.8349e-01, 4.0341e-01,\n",
      "        2.6148e-01, 1.2424e+00, 7.5073e-02, 8.1088e-02, 1.3519e+00, 9.1690e-01,\n",
      "        3.9431e-02, 3.7708e-01, 1.7585e+00, 4.4842e-01, 3.9821e-01, 3.6077e-01,\n",
      "        9.7705e-01, 6.9266e-01, 6.4223e-01, 1.7649e+00, 1.0529e-01, 9.5751e-01,\n",
      "        5.6519e-01, 1.4064e-01, 2.9176e-01, 7.0234e-01, 2.2236e-01, 3.8264e-02,\n",
      "        9.1057e-01, 6.1459e-01, 3.9115e-01, 1.0555e-01, 1.1037e+00, 4.4490e-01,\n",
      "        5.6297e-01, 1.1013e+00, 7.0639e-01, 4.0298e-02, 6.7317e-01, 9.1655e-01,\n",
      "        5.9270e-01, 7.7877e-01, 5.1742e-02, 1.3579e-01, 1.3163e+00, 4.1447e-01,\n",
      "        6.3269e-01, 7.0438e-02, 1.3338e+00, 1.8937e-01, 3.2501e-01, 5.8101e-01,\n",
      "        1.3411e-01, 1.6261e-01, 7.4628e-01, 9.9760e-01, 9.4337e-02, 6.7948e-01,\n",
      "        1.8297e-01, 5.4644e-01, 7.8763e-01, 1.1144e+00, 2.6790e-01, 8.8858e-01,\n",
      "        2.0045e+00, 7.6746e-01, 6.8928e-01, 2.8957e-01, 5.1223e-01, 3.2734e-01,\n",
      "        5.4822e-01, 2.0978e+00, 3.3893e-01, 1.4505e-01, 1.0428e-01, 1.8292e-01,\n",
      "        1.0493e+00, 2.6773e-01, 2.5462e-01, 7.5169e-01, 1.5631e-01, 7.5948e-01,\n",
      "        1.1135e+00, 5.6013e-01, 2.6218e-01, 6.3303e-01, 5.7903e-01, 7.0778e-01,\n",
      "        1.8023e-01, 1.0111e+00, 2.4711e-01, 2.6107e-01, 9.1166e-01, 9.9638e-01,\n",
      "        2.8284e-01, 2.2672e-01, 3.2048e-01, 6.8218e-01, 2.5453e-01, 7.4655e-01,\n",
      "        1.7998e-01, 1.1030e+00, 1.3836e+00, 6.2955e-01, 6.2815e-02, 2.2085e-01,\n",
      "        9.9471e-01, 1.3865e-01, 1.8148e+00, 7.2738e-01, 8.7992e-01, 5.8968e-01,\n",
      "        5.8652e-01, 2.2547e-02, 2.3347e+00, 1.0705e-01, 3.1678e+00, 8.8123e-03,\n",
      "        1.3105e+00, 3.6242e-02, 8.6724e-01, 1.3707e+00, 8.7496e-01, 1.1584e+00,\n",
      "        1.3434e-01, 1.2488e+00, 3.8676e-01, 7.5586e-01, 1.3605e+00, 7.1131e-01,\n",
      "        8.5230e-01, 5.0145e-01, 7.7843e-01, 5.6135e-01, 6.6616e-01, 1.8539e+00,\n",
      "        1.7722e-01, 1.0690e+00, 6.0987e-01, 1.1963e+00, 6.0859e-01, 2.6147e-01,\n",
      "        1.3850e+00, 7.9732e-01, 4.7997e-01, 1.9481e+00, 9.2436e-01, 6.8382e-02,\n",
      "        1.9602e+00, 5.7410e-01, 7.7569e-01, 3.6824e-01, 9.4233e-02, 1.1855e+00,\n",
      "        4.5004e-01, 3.8699e-01, 5.5451e-01, 1.2740e+00, 6.0985e-01, 6.6221e-01,\n",
      "        7.8207e-02, 3.7348e-01, 4.2298e-01, 2.6192e-01, 2.5512e-01, 3.9916e-01,\n",
      "        6.8960e-01, 2.2682e-02, 1.0879e+00, 8.7622e-02, 6.0176e-01, 7.5113e-01,\n",
      "        9.0410e-02, 2.2768e-01, 6.1994e-01, 1.9141e-01, 4.5728e-01, 5.3628e-01,\n",
      "        5.1149e-01, 8.5386e-02, 2.5664e-01, 4.1782e-01, 4.0646e-01, 2.9412e-01,\n",
      "        2.6603e-01, 3.8213e-02, 8.7337e-01, 4.7538e-01, 2.7382e-01, 4.2270e-01,\n",
      "        4.8860e-01, 1.1714e+00, 5.9338e-02, 2.5214e-01, 1.2117e-01, 9.3003e-01,\n",
      "        8.2440e-01, 3.7790e-01, 1.1441e+00, 1.4989e+00, 5.3185e-01, 1.4816e+00,\n",
      "        3.4659e-01, 2.7584e-01, 1.4356e-01, 1.4467e+00, 4.5015e-01, 5.4210e-01,\n",
      "        1.4965e-01, 1.8073e+00, 2.4736e-01, 9.3415e-01, 2.2440e-01, 8.5739e-01,\n",
      "        1.1091e-01, 3.2383e-01, 6.6050e-02, 6.8104e-02, 9.7989e-01, 5.3433e-01,\n",
      "        7.7279e-01, 1.5967e-01, 1.1298e+00, 4.0054e-01, 6.9831e-01, 7.7493e-01,\n",
      "        9.5973e-02, 6.3851e-01, 1.4386e+00, 1.1566e+00, 3.8189e-01, 4.9318e-01,\n",
      "        9.5232e-02, 1.4426e-01, 2.5910e-02, 3.0549e-02, 5.7549e-01, 6.2886e-01,\n",
      "        2.9116e-01, 1.5590e+00, 3.6730e-01, 3.5595e-01, 2.4795e-01, 3.5964e-01,\n",
      "        2.3540e+00, 3.4370e-01, 5.1058e-01, 9.2361e-02, 2.1978e-01, 7.6361e-01,\n",
      "        8.7558e-01, 3.9460e-01, 6.9728e-01, 2.5736e-01, 2.1905e+00, 7.5499e-01,\n",
      "        1.1618e-01, 2.2154e-01, 3.0171e-01, 2.8600e-01, 3.1989e-01, 1.2603e-01,\n",
      "        4.2908e-01, 1.2696e+00, 1.4848e+00, 5.8165e-01, 2.3164e-01, 3.2020e-01,\n",
      "        1.2508e+00, 1.0756e+00, 6.7063e-01, 8.3704e-01, 4.8116e-01, 7.0230e-01,\n",
      "        1.0621e-01, 8.2496e-01, 5.5748e-01, 7.2114e-01, 1.2964e-01, 4.4469e-01,\n",
      "        5.8240e-01, 8.3735e-01, 2.4377e-01, 6.1536e-01, 7.4485e-01, 1.2401e-01,\n",
      "        2.1167e+00, 1.3404e-01, 5.9433e-01, 3.2866e-01, 9.0177e-02, 8.3866e-01,\n",
      "        1.4433e+00, 1.1024e+00, 5.9231e-01, 1.2122e-01, 5.0731e-01, 1.1744e-01,\n",
      "        1.9564e-01, 4.8017e-01])\n",
      "The output size: is torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86634553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is:  tensor([1.5335, 0.4238, 0.4668, 0.5618, 0.8520, 0.2227, 0.2807, 0.3332, 0.4589,\n",
      "        0.3058, 1.0729, 1.4955, 0.4570, 0.7664, 0.8046, 0.7344, 0.6005, 1.7180,\n",
      "        1.6090, 0.4628, 0.7581, 0.2809, 0.4536, 0.7133, 0.8271, 0.7206, 0.7130,\n",
      "        0.5719, 0.5614, 0.9395, 0.3142, 1.0028, 0.4288, 0.9812, 0.9470, 0.5246,\n",
      "        0.2880, 0.9706, 0.4048, 1.5862, 0.2256, 0.7025, 0.3525, 1.1178, 1.9905,\n",
      "        0.7110, 0.7580, 0.2655, 0.2853, 1.2196, 1.6573, 0.8917, 1.4085, 0.7297,\n",
      "        0.5269, 0.8930, 2.3949, 0.4386, 0.1402, 0.5878, 0.7193, 0.5694, 0.4566,\n",
      "        0.8101, 2.4654, 1.4529, 1.3236, 1.1483, 1.3578, 0.8419, 0.5992, 0.6983,\n",
      "        0.6348, 1.2482, 0.6887, 1.8605, 0.6350, 0.2882, 0.1456, 0.7113, 0.3922,\n",
      "        0.7331, 0.7470, 0.5255, 0.1934, 0.4903, 1.0999, 0.6590, 0.5998, 1.2706,\n",
      "        1.3048, 1.3398, 1.1573, 0.2108, 1.5209, 0.7340, 0.3765, 0.8705, 0.8831,\n",
      "        0.4856, 1.7266, 0.2835, 0.2238, 1.8971, 0.3803, 1.2640, 1.3930, 3.3544,\n",
      "        1.0169, 1.1262, 0.9027, 1.5010, 0.4326, 2.1800, 1.8048, 0.7227, 0.3325,\n",
      "        0.3644, 1.6612, 0.5376, 0.6075, 0.7657, 0.6520, 0.2167, 0.6390, 0.7087,\n",
      "        1.5632, 0.4069, 1.9590, 0.7522, 1.3485, 0.9425, 0.3205, 1.4220, 0.2948,\n",
      "        0.3100, 0.4660, 1.1387, 0.5384, 0.4561, 0.5627, 1.9072, 0.1727, 1.0301,\n",
      "        0.5751, 0.0689, 0.3409, 2.2975, 1.0244, 0.2689, 0.2617, 0.3422, 0.9562,\n",
      "        1.1524, 1.9831, 1.2659, 1.4336, 0.9490, 0.6056, 0.6383, 0.2250, 0.5074,\n",
      "        0.6295, 0.6422, 3.4389, 0.3390, 0.3530, 0.8757, 1.6675, 0.5805, 0.6662,\n",
      "        1.2041, 0.7216, 0.4868, 0.6374, 0.7890, 0.6738, 0.8773, 0.5687, 0.6017,\n",
      "        0.5113, 2.8874, 0.1848, 1.5386, 0.5740, 0.8593, 1.7299, 1.3647, 0.6176,\n",
      "        0.2805, 0.5743, 0.9039, 1.3156, 0.5566, 0.9399, 1.2922, 0.5470, 1.3822,\n",
      "        2.4383, 0.8020, 0.7587, 0.5412, 0.8963, 0.5440, 0.5419, 0.4084, 0.5552,\n",
      "        0.7307, 0.4929, 0.7429, 1.2259, 0.8788, 0.2510, 1.1077, 0.5113, 0.4983,\n",
      "        0.2374, 0.9613, 0.6584, 0.0964, 0.4661, 0.2480, 0.4626, 1.8013, 1.1749,\n",
      "        0.5110, 0.5817, 0.2414, 1.0731, 0.6968, 1.2046, 1.4256, 0.4930, 0.9254,\n",
      "        0.6156, 0.7063, 0.3873, 0.4379, 1.8699, 0.4049, 0.6782, 1.5406, 0.9715,\n",
      "        0.1964, 2.1977, 1.1341, 1.0570, 0.8530, 1.4424, 1.3172, 2.2454, 0.7268,\n",
      "        0.7287, 0.8926, 0.2599, 0.3802, 0.7545, 0.4421, 0.9912, 0.2445, 1.3455,\n",
      "        0.5730, 0.7404, 0.4794, 0.7035, 0.7409, 0.6702, 1.3825, 0.4512, 1.5883,\n",
      "        1.0827, 2.5044, 1.2151, 1.5860, 0.6625, 0.3499, 1.1251, 0.6642, 1.1461,\n",
      "        1.0866, 1.3325, 0.4949, 0.2290, 1.9110, 0.7852, 1.5893, 0.3299, 0.1351,\n",
      "        0.7901, 0.0587, 0.7141, 0.6856, 0.4622, 1.1573, 2.8628, 0.2687, 0.3125,\n",
      "        0.5013, 0.5233, 0.7526, 0.3184, 0.9485, 1.0255, 0.0083, 2.3800, 0.3279,\n",
      "        1.1383, 0.5352, 1.4454, 0.9581, 0.2711, 1.1979, 0.6272, 1.4422, 1.8476,\n",
      "        1.1935, 0.3372, 0.7880, 1.2937, 0.4111, 1.6749, 0.8633, 0.4146, 1.7352,\n",
      "        0.6756, 0.0078, 2.6003, 0.1006, 2.6064, 0.7624, 0.7687, 0.3999, 0.6395,\n",
      "        1.1489, 1.2181, 0.6162, 1.1250, 2.3111, 0.5892, 0.8852, 2.5225, 1.6286,\n",
      "        0.9708, 1.2423, 1.0959, 0.7481, 1.9997, 1.3786, 0.2832, 0.9059, 1.7134,\n",
      "        1.4126, 0.9339, 0.4737, 0.6005, 1.0236, 0.2019, 0.9291, 0.9609, 0.6330,\n",
      "        2.2480, 0.5148, 1.1513, 0.5871, 1.3963, 2.0018, 0.3669, 0.6709, 1.3750,\n",
      "        1.7204, 1.4859, 0.8162, 0.1198, 0.7074, 0.5000, 0.6582, 1.1671, 0.6570,\n",
      "        0.7593, 0.5880, 1.5009, 0.8969, 1.6077, 1.2060, 0.3097, 1.2859, 1.0629,\n",
      "        0.7870, 2.1945, 1.5254, 0.2736, 0.2910, 0.4974, 0.8501, 0.9007, 0.0500,\n",
      "        0.8398, 0.1156, 0.6776, 1.3428, 0.4943, 0.5060, 0.0808, 1.6539, 0.3049,\n",
      "        0.6862, 0.4809, 1.1898, 0.5681, 0.5194, 0.5667, 1.1836, 0.3257, 1.0107,\n",
      "        0.3352, 0.3032, 1.0568, 0.8495, 0.7136, 0.3562, 0.6284, 1.2784, 0.4667,\n",
      "        1.6545, 0.2254, 2.3463, 0.4764, 0.3662, 0.4869, 0.8007, 1.9644, 1.7471,\n",
      "        1.8397, 0.3141, 2.0064, 0.8434, 0.7001, 0.9353, 0.2387, 1.3149, 0.9976,\n",
      "        1.7883, 0.4826, 1.8090, 0.5184, 0.3189, 0.0287, 0.3790, 1.0392, 0.8504,\n",
      "        0.3024, 2.5001, 0.5544, 0.2099, 0.6163, 1.3794, 1.3397, 0.2359, 0.7394,\n",
      "        0.4039, 0.2355, 1.3043, 0.5863, 0.6572, 1.2804, 1.1371, 1.2135, 0.3739,\n",
      "        0.3482, 0.3739, 0.7912, 0.5985, 0.2059, 0.6527, 0.8591, 2.3255, 1.7848,\n",
      "        0.4654, 0.4340, 1.0018, 2.4553, 0.5943, 0.5484, 1.8260, 1.7181, 0.4568,\n",
      "        0.9658, 2.6766, 0.9168, 0.4537, 0.1852, 1.8491, 0.0407, 0.2146, 0.1660,\n",
      "        1.2213, 0.5941, 0.6126, 1.3604, 0.4480, 0.5110, 0.3984, 0.6098, 0.9213,\n",
      "        1.9152, 1.4629, 0.6957, 0.8040, 0.8031, 0.6851, 0.2447, 0.2197])\n",
      "The output size: is torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099ec103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is:  tensor([0.9367, 0.4827, 1.0497, 0.4203, 0.3126, 0.4230, 1.2803, 0.3168, 0.6442,\n",
      "        0.9106, 0.7423, 2.5590, 1.0426, 1.0067, 0.9097, 0.2099, 0.2941, 0.4455,\n",
      "        0.3833, 0.5824, 0.0207, 0.4135, 0.8301, 0.8446, 0.5102, 1.0307, 0.7937,\n",
      "        0.3483, 0.9858, 0.6824, 0.1545, 0.6911, 0.7652, 1.0923, 1.2133, 1.5107,\n",
      "        0.3752, 2.1716, 0.9251, 0.7581, 0.3904, 0.1953, 0.2319, 1.0024, 1.1077,\n",
      "        0.4836, 0.6344, 0.9908, 0.7618, 2.8145, 0.5304, 1.2291, 1.4844, 1.0228,\n",
      "        0.2132, 0.5384, 0.4033, 0.5564, 0.7473, 0.8041, 0.1867, 1.1768, 0.1432,\n",
      "        0.5795, 0.7811, 0.0133, 0.7557, 1.0523, 1.9415, 0.7934, 0.2676, 1.0487,\n",
      "        1.9105, 2.5016, 1.7352, 0.1147, 0.1244, 0.2670, 0.1145, 0.2925, 0.3337,\n",
      "        0.9030, 0.9493, 1.3562, 0.2458, 1.0983, 1.0096, 1.2707, 0.8021, 1.8631,\n",
      "        0.7592, 0.5140, 0.3852, 0.8677, 0.7106, 1.4081, 0.3101, 0.3991, 0.9322,\n",
      "        1.0870, 0.2850, 0.5233, 0.3938, 2.3760, 1.3855, 2.0014, 0.7741, 1.7549,\n",
      "        0.3006, 0.7468, 0.0223, 0.9510, 0.4350, 1.2673, 0.9652, 1.6581, 0.8418,\n",
      "        0.1723, 1.1296, 1.3268, 0.0872, 0.5253, 0.6316, 1.4987, 0.4356, 0.5121,\n",
      "        0.2422, 0.0507, 0.4875, 0.7172, 0.9971, 0.1613, 0.5199, 1.8544, 1.1021,\n",
      "        1.4755, 0.8438, 0.2733, 0.7590, 0.0855, 0.7833, 1.0038, 0.6156, 0.7364,\n",
      "        0.7955, 0.1036, 0.7329, 1.9173, 0.8855, 0.0631, 0.4627, 0.2047, 0.9929,\n",
      "        0.5724, 0.3505, 0.6554, 0.4305, 0.1074, 0.5443, 0.5309, 0.6393, 0.4395,\n",
      "        0.1313, 0.1408, 1.2759, 0.2719, 0.5835, 0.4592, 0.1493, 0.6984, 0.8210,\n",
      "        1.1920, 0.6473, 0.4599, 0.0765, 0.8444, 0.5507, 0.3346, 1.7194, 0.0051,\n",
      "        0.7227, 1.7318, 0.3325, 0.8800, 0.7697, 0.0583, 0.6320, 0.2417, 0.1330,\n",
      "        0.0900, 0.5023, 1.1556, 0.7945, 0.6777, 0.9416, 0.9314, 0.2949, 0.9925,\n",
      "        2.9900, 1.1589, 0.8009, 0.3715, 1.3614, 0.1129, 1.9141, 0.8194, 0.9116,\n",
      "        0.5159, 0.8065, 0.5745, 0.9489, 0.3015, 0.4651, 3.1661, 1.0365, 0.1852,\n",
      "        0.0803, 0.9185, 0.3466, 0.7288, 1.8984, 1.6442, 0.5026, 1.3802, 1.7508,\n",
      "        0.8012, 0.4661, 0.1023, 0.2429, 1.1034, 0.7574, 1.5004, 0.3619, 0.2715,\n",
      "        0.9359, 0.4095, 0.3095, 2.0019, 1.5818, 0.0845, 2.3352, 1.1458, 1.4300,\n",
      "        2.2718, 0.1260, 0.3796, 0.6724, 0.6027, 0.1812, 0.2443, 0.3244, 1.1935,\n",
      "        0.4170, 0.5230, 0.5255, 0.3075, 0.3140, 0.1023, 0.1722, 0.7085, 0.5723,\n",
      "        0.9361, 0.0092, 0.2569, 1.9874, 0.1801, 1.1171, 0.5139, 1.6033, 0.6138,\n",
      "        0.8880, 0.8311, 0.5899, 1.0455, 0.1167, 0.7550, 1.8487, 1.0486, 0.1512,\n",
      "        0.3645, 2.5716, 1.1689, 0.2481, 1.2156, 2.4775, 1.3333, 0.4828, 0.6415,\n",
      "        1.0060, 0.1158, 0.2508, 0.2384, 0.8595, 0.3581, 0.3661, 0.8032, 0.0597,\n",
      "        1.6066, 1.5509, 1.1581, 0.5037, 0.6888, 0.7432, 0.1417, 0.7572, 0.4204,\n",
      "        1.3441, 0.5536, 1.5926, 1.3141, 1.1197, 0.6890, 0.2004, 0.5556, 2.0727,\n",
      "        0.4399, 0.2553, 1.1788, 1.2856, 0.2093, 3.6023, 2.0037, 0.5838, 2.5186,\n",
      "        1.2824, 0.4956, 1.8900, 0.7222, 1.3565, 0.8000, 0.5538, 0.2677, 0.9881,\n",
      "        0.7137, 0.9791, 0.4760, 1.4209, 1.1065, 1.0632, 0.7790, 1.7809, 0.0943,\n",
      "        1.8932, 1.7933, 0.4632, 0.9899, 1.8422, 0.4624, 0.6659, 0.5537, 0.9030,\n",
      "        1.4047, 0.9525, 0.2982, 0.6683, 1.1628, 0.6319, 0.8285, 1.3447, 0.1723,\n",
      "        1.5444, 0.2768, 1.3761, 0.5259, 1.1226, 1.0510, 0.2649, 0.2152, 0.7347,\n",
      "        1.1471, 0.6070, 0.8866, 0.2892, 0.4140, 0.4914, 0.7607, 0.1996, 0.0187,\n",
      "        1.3890, 0.4726, 0.7826, 0.4491, 1.2489, 2.0784, 0.4796, 0.1789, 0.8979,\n",
      "        0.4368, 1.3226, 0.9622, 0.4105, 0.4860, 0.6464, 0.7005, 0.9958, 0.4201,\n",
      "        0.9721, 0.0504, 1.5893, 1.6183, 0.2742, 1.2421, 0.4645, 1.5099, 0.0921,\n",
      "        0.3902, 0.0316, 0.9919, 0.2305, 0.2114, 0.8346, 0.2746, 0.6592, 0.9353,\n",
      "        0.2139, 1.1212, 1.1753, 1.1456, 0.1404, 0.2599, 0.5011, 0.6463, 1.1406,\n",
      "        1.0827, 0.1708, 1.4834, 1.1235, 0.0621, 0.3979, 1.8255, 1.4775, 1.1972,\n",
      "        1.2829, 0.1642, 0.9397, 0.1591, 0.2751, 0.4842, 0.4585, 1.8931, 0.7967,\n",
      "        1.7550, 0.7022, 1.8260, 0.2711, 0.3447, 0.3948, 0.3484, 0.0336, 1.5022,\n",
      "        0.2620, 2.6037, 2.0490, 1.0803, 0.9049, 0.3076, 1.5694, 0.9732, 0.6007,\n",
      "        1.0895, 0.0415, 0.6835, 0.8585, 0.1799, 0.2409, 0.9185, 1.9297, 0.1107,\n",
      "        0.3751, 0.0135, 0.3846, 0.4276, 0.6155, 0.4991, 0.7388, 0.8524, 0.4091,\n",
      "        1.0062, 0.9677, 1.6288, 1.9786, 0.5334, 1.1182, 1.2415, 0.5000, 0.1940,\n",
      "        1.6368, 1.6449, 2.2013, 1.0446, 0.8019, 0.7068, 1.4805, 0.3624, 0.4599,\n",
      "        0.3416, 0.4430, 0.1325, 1.0248, 0.3281, 0.3701, 0.2818, 0.2907, 0.7026,\n",
      "        1.2272, 0.5634, 0.0540, 0.1594, 0.3825, 0.6647, 0.6317, 1.3497])\n",
      "The output size: is torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c96dde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is:  tensor([1.4409, 0.2856, 1.0468, 1.3077, 1.1834, 1.2234, 1.2630, 1.0709, 0.4479,\n",
      "        2.0818, 1.0171, 0.7398, 0.9103, 1.4350, 0.3696, 1.0779, 1.4036, 0.7182,\n",
      "        2.2314, 1.1611, 0.6076, 0.9305, 0.9713, 1.9461, 0.4874, 0.8314, 0.4516,\n",
      "        0.6637, 0.7863, 1.1021, 0.1933, 0.2794, 1.8596, 0.7272, 0.8621, 0.5504,\n",
      "        0.7251, 0.5561, 0.4284, 1.9499, 0.8450, 1.1227, 0.3555, 1.7234, 1.5830,\n",
      "        1.0052, 0.4463, 0.1038, 0.8854, 0.3777, 1.3387, 2.2872, 1.2170, 0.9491,\n",
      "        1.3649, 1.1907, 1.7864, 0.3595, 1.0697, 1.4559, 0.4935, 0.8914, 0.3439,\n",
      "        1.8230, 0.8692, 1.4212, 0.7097, 1.3653, 1.5365, 0.9737, 0.3791, 0.6992,\n",
      "        1.5996, 0.3821, 0.7864, 1.1483, 0.8616, 0.7107, 1.1375, 0.6962, 1.7805,\n",
      "        2.6542, 1.3253, 0.6685, 1.1710, 0.2853, 2.0289, 0.4045, 1.2759, 0.7948,\n",
      "        1.1902, 0.6980, 0.4122, 0.5687, 0.7518, 0.4956, 1.9488, 1.7498, 0.8833,\n",
      "        1.0536, 0.0978, 1.4926, 0.4372, 0.8673, 0.2266, 0.5248, 1.4031, 1.2539,\n",
      "        0.8904, 1.0884, 1.8828, 1.6514, 0.2608, 1.1906, 0.2880, 1.5138, 0.8608,\n",
      "        0.1760, 1.2533, 1.7531, 0.3212, 0.5116, 0.3118, 0.6804, 1.6985, 1.3726,\n",
      "        1.0060, 1.8567, 1.6224, 0.4720, 2.6982, 1.1584, 0.6487, 1.8532, 2.1377,\n",
      "        0.3604, 0.8005, 0.7019, 0.5050, 0.2658, 0.5092, 1.6380, 1.7193, 0.7421,\n",
      "        0.8523, 1.8582, 0.2517, 1.0077, 1.0351, 1.4689, 1.0077, 1.6073, 0.5079,\n",
      "        0.9044, 0.3787, 0.8769, 1.0936, 1.0509, 1.2514, 2.0409, 0.2103, 1.9118,\n",
      "        0.6987, 0.8645, 2.0535, 0.5838, 0.7039, 1.8774, 0.4962, 2.0012, 0.5510,\n",
      "        0.7481, 0.2246, 1.2589, 0.0652, 0.0798, 1.4004, 0.5519, 0.1760, 0.6625,\n",
      "        0.2948, 2.0323, 0.5001, 1.3960, 0.0751, 1.6375, 0.7994, 0.6250, 1.0997,\n",
      "        0.5422, 0.4178, 1.7325, 0.4972, 1.1106, 0.5834, 1.7283, 0.6688, 0.4978,\n",
      "        0.8366, 0.7904, 2.1242, 1.5670, 1.0350, 1.0120, 2.3929, 0.2727, 0.6161,\n",
      "        1.8346, 0.6179, 0.4976, 2.1621, 1.5448, 1.0478, 1.3624, 1.1745, 1.6275,\n",
      "        1.7117, 0.4495, 0.5899, 0.3900, 0.5309, 0.6004, 0.6051, 1.7879, 1.8531,\n",
      "        0.7400, 2.4792, 0.5579, 2.2881, 2.5066, 0.9047, 0.9133, 1.0847, 0.4937,\n",
      "        0.5944, 1.2953, 0.5933, 1.6606, 1.8787, 1.3398, 0.8925, 2.0176, 1.1623,\n",
      "        0.5343, 1.3027, 1.0059, 1.2568, 0.5393, 1.1900, 2.7539, 1.0576, 2.0538,\n",
      "        0.3810, 0.9831, 0.7418, 0.9435, 2.3449, 0.4848, 0.6565, 0.4537, 2.6869,\n",
      "        1.0927, 0.1426, 1.1339, 0.2849, 1.9873, 0.6881, 1.8562, 1.5393, 1.6477,\n",
      "        0.1842, 1.9295, 1.6698, 1.1766, 1.5669, 0.9502, 0.8964, 1.6690, 0.7893,\n",
      "        1.3169, 1.9849, 1.1700, 0.8999, 0.8724, 1.5067, 0.3831, 0.3495, 0.2202,\n",
      "        0.3719, 1.0280, 1.5869, 0.6226, 0.8645, 1.2328, 2.4821, 0.9543, 0.8703,\n",
      "        1.2504, 1.2714, 1.7650, 1.0745, 1.1341, 1.9488, 0.2771, 0.5460, 1.7183,\n",
      "        0.6526, 1.3460, 1.4320, 0.9884, 0.7641, 2.0812, 2.3484, 0.6552, 2.6137,\n",
      "        0.7348, 0.1190, 0.5095, 0.6097, 0.3078, 1.4484, 2.5164, 0.6198, 2.4131,\n",
      "        0.5859, 0.7290, 0.9748, 1.0186, 2.1096, 1.0105, 0.4459, 0.9655, 1.0302,\n",
      "        1.0095, 1.6590, 0.6984, 0.5658, 2.4015, 0.8051, 0.6357, 1.4387, 0.7629,\n",
      "        0.2545, 1.1567, 1.1122, 0.5580, 1.5725, 0.5617, 0.1410, 0.8251, 2.2575,\n",
      "        1.5882, 1.1282, 0.2545, 0.2872, 1.3746, 1.6234, 0.5229, 1.5041, 0.8799,\n",
      "        0.2358, 0.8585, 0.6421, 0.1699, 0.9532, 1.1900, 0.5111, 1.0168, 2.0041,\n",
      "        1.8232, 1.2468, 0.9358, 0.5796, 0.5629, 0.3115, 0.7465, 1.5021, 0.5422,\n",
      "        0.7338, 0.9020, 2.5817, 1.1925, 1.3676, 1.7413, 0.5584, 4.8162, 0.7155,\n",
      "        0.3386, 1.0047, 0.6670, 0.3163, 1.4466, 0.3032, 0.8711, 0.7030, 1.7210,\n",
      "        0.9395, 0.2305, 1.2384, 1.2319, 0.5451, 1.7415, 0.6723, 2.1507, 0.5409,\n",
      "        0.5435, 0.6064, 0.7016, 2.4059, 0.6240, 0.5082, 1.5303, 0.5796, 0.6784,\n",
      "        0.6293, 0.8500, 0.1910, 0.7415, 0.5731, 0.5786, 1.2030, 1.8975, 1.0476,\n",
      "        1.1168, 0.5992, 1.0195, 0.9790, 1.3199, 0.1969, 0.8553, 1.1111, 1.5389,\n",
      "        1.8882, 0.8245, 1.4095, 1.1619, 1.3468, 0.1088, 1.1434, 1.2802, 0.5668,\n",
      "        2.7556, 0.5445, 2.6757, 0.3214, 0.6481, 0.4034, 0.6522, 0.7041, 1.3988,\n",
      "        0.1963, 0.5829, 1.2349, 0.5526, 0.3698, 1.5004, 1.4827, 0.7871, 0.5914,\n",
      "        1.4255, 0.3766, 2.0559, 1.3167, 2.0932, 0.4053, 1.0576, 1.7921, 1.7932,\n",
      "        1.3255, 1.5752, 0.6039, 1.5090, 1.5804, 0.4917, 1.5516, 0.9611, 1.7614,\n",
      "        0.9142, 0.4378, 0.5808, 0.5631, 0.3985, 1.1326, 0.4637, 1.9831, 1.0838,\n",
      "        1.3484, 1.0001, 1.4835, 0.9675, 0.3480, 0.9936, 0.7300, 0.3518, 0.7990,\n",
      "        1.1877, 0.6468, 1.1225, 1.2957, 0.6881, 1.0606, 1.2561, 0.1979, 1.4993,\n",
      "        2.0018, 1.7366, 0.9258, 0.2049, 0.3066, 0.9971, 0.4254, 0.6035])\n",
      "The output size: is torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "frames_collect(sample_input_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c76081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
