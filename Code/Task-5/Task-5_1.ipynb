{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d675d305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srirupin/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/srirupin/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "#Import the models\n",
    "model = models.video.r3d_18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "#for the shortlisted m videos using opencv the videos are visualized\n",
    "def visualise(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        cv2.imshow(\"The captured frame is: \", frame)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() \n",
    "\n",
    "\n",
    "#Defining the hook for the output\n",
    "def output_hook(desired_layers):\n",
    "    layer_outputs = {}\n",
    "     # Gather the output for modules for the specified layers\n",
    "    def hook_function(module, input, output):\n",
    "        for name, mod in model.named_modules():\n",
    "            if mod == module:\n",
    "                module_name = name\n",
    "        if module_name in desired_layers:\n",
    "            layer_outputs[module_name] = output.detach()\n",
    "    # Registering the hook for the layer in desired_layer\n",
    "    for name, module in model.named_modules():\n",
    "        if name in desired_layers:\n",
    "            module.register_forward_hook(hook_function)\n",
    "    return layer_outputs\n",
    "\n",
    "#Sliding Window Technique with window_size and step\n",
    "def sliding_window(frames, window_size, step):\n",
    "    slided_frames = []\n",
    "    total_frames = len(frames)\n",
    "    for i in range(0, total_frames - window_size +1, step):\n",
    "        current_slide =[]\n",
    "        for j in range(window_size):\n",
    "            current_slide.append(frames[i+j])\n",
    "        slided_frames.append(current_slide)\n",
    "    return slided_frames\n",
    "\n",
    "# Function to preprocess the video and apply sliding window approach  \n",
    "def frames_collect(video_file_path):    \n",
    "    #Capture the video\n",
    "    desired_layers = ['layer3']\n",
    "    layer_outputs = output_hook(desired_layers)\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #Defining the maximum frames to 32 and step as 16\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    # Get sequences using sliding window\n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skip the video\")\n",
    "        return\n",
    "    \n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        # Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    # Stack outputs along a new dimension\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    fully_connected = nn.Linear(256, 512)\n",
    "    output_current_layer = fully_connected(output_current_layer)\n",
    "    return output_current_layer.tolist()\n",
    "\n",
    "\n",
    "# Function to preprocess the video and apply sliding window approach  \n",
    "def frames_collect_2(video_file_path, desired_layer):    \n",
    "    #Capture the video\n",
    "    layer_outputs = output_hook(desired_layer)\n",
    "    video_frames = []\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Couldnot read the video\")\n",
    "        return\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames.append(frame)\n",
    "        # Visualizing the video frame using OpenCV\n",
    "        #cv2.imshow(\"The captured frame is: \", frame)\n",
    "        #if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "    \n",
    "    # Release and destroy the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()    \n",
    "    #Defining the maximum frames to 32 and step as 16\n",
    "    max_frames = 32\n",
    "    step = 16\n",
    "    sliding_frames = []\n",
    "    # Get sequences using sliding window\n",
    "    if len(video_frames)>=32:\n",
    "        sliding_frames = sliding_window(video_frames, max_frames, step)\n",
    "    else:\n",
    "        print(\"Skip the video\")\n",
    "        return\n",
    "    \n",
    "    # Transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    #Gathering all layer outputs ffrom the sliding window function\n",
    "    slided_output_layer = []\n",
    "    transformed_frame=[]\n",
    "    for frames in sliding_frames:\n",
    "        transformed_frame=[]\n",
    "        for frame in frames:\n",
    "            transform_frame=transform(frame)\n",
    "            transformed_frame.append(transform_frame)\n",
    "        tensor_frames = torch.stack(transformed_frame)\n",
    "        tensor_frames = tensor_frames.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "        # Append the layers\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_frames)\n",
    "        for layer_name, output in layer_outputs.items():\n",
    "            slided_output_layer.append(output)\n",
    "    # Stack outputs along a new dimension\n",
    "    stack_layers = torch.stack(slided_output_layer, dim=0)\n",
    "    # Apply max pooling across the windows (dim=0)\n",
    "    maxpooling_output = torch.max(stack_layers, dim=0).values\n",
    "    maxpooling_output = maxpooling_output.squeeze(0)\n",
    "    output_current_layer = maxpooling_output.mean(dim=[1,2,3])\n",
    "    return output_current_layer.tolist()\n",
    "\n",
    "\n",
    "#Read Json\n",
    "def read_json_file(file_name):\n",
    "    with open(file_name, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "#distance function - manhattan \n",
    "def manhattan(a, b):\n",
    "    res=0\n",
    "    for i in range(0,3):\n",
    "        res = res + abs(a[i]-b[i])\n",
    "    return res\n",
    "\n",
    "res=[]\n",
    "#Define Nearest Search\n",
    "def nearest_search(video_file_path):\n",
    "    sample_layer3 = frames_collect(video_file_path)\n",
    "    sample_layer4 = frames_collect_2(video_file_path, ['layer4'] )\n",
    "    sample_avgpool = frames_collect_2(video_file_path, ['avgpool'])\n",
    "    #Define the Json Files to read\n",
    "    json_files = [\"sword_output.json\", \"cartwheel_output.json\", \"drink_output.json\",\n",
    "                 \"ridebikeoutput.json\", \"swordexcercise_output.json\", \"wave_output.json\"]\n",
    "    #Loop throught the json files\n",
    "    for k in range(0, len(json_files)):\n",
    "        data = read_json_file(json_files[k])\n",
    "        layer_3 = data[0]\n",
    "        layer_4 = data[1]\n",
    "        layer_5 = data[2]\n",
    "        #On every layer calculate the manhattan distance\n",
    "        for i in range(0, len(layer_3)):\n",
    "            curr_1 = layer_3[i]\n",
    "            curr_2 = layer_4[i]\n",
    "            curr_3 = layer_5[i]\n",
    "            dist=0\n",
    "            for j in range(0, len(curr_1)):\n",
    "                curr_sum=0\n",
    "                a=[curr_1[j], curr_2[j], curr_3[j]]\n",
    "                b=[sample_layer3[j], sample_layer4[j], sample_avgpool[j]]\n",
    "                curr_sum += manhattan(a,b)\n",
    "                dist=curr_sum\n",
    "            res.append((dist, i, json_files[k]))\n",
    "        #return res\n",
    "    #Sort the result\n",
    "    res.sort(key=lambda i:i[0])\n",
    "    #Gather the similarity Videos\n",
    "    similar_videos = []\n",
    "    #Visualise the videos\n",
    "    for i in range(0,10):\n",
    "        print(res[i])\n",
    "        similar_videos.append(res[i])\n",
    "    file_path = {'sword_output.json':'/Users/srirupin/Downloads/target/sword' ,\n",
    "            'cartwheel_output.json': '/Users/srirupin/Downloads/target/cartwheel',\n",
    "            'ridebikeoutput.json':'/Users/srirupin/Downloads/target/ride_bike',\n",
    "            'swordexcercise_output.json': '/Users/srirupin/Downloads/target/sword_exercise',\n",
    "            'wave_output.json': '/Users/srirupin/Downloads/target/wave',\n",
    "            'drink_output.json': '/Users/srirupin/Downloads/target/drink' \n",
    "            }\n",
    "    output=[]\n",
    "    for i in range(0,10):\n",
    "        video_file_path = file_path[res[i][2]]\n",
    "        j=0\n",
    "        for k in os.listdir(video_file_path):\n",
    "            if j ==res[i][1]:\n",
    "                path = f'{video_file_path}/{k}'\n",
    "                print(\"The Distance Measure: \", res[i][0])\n",
    "                visualise(path)\n",
    "                output.append((k, res[i][0]))\n",
    "            j=j+1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dcd12d7",
   "metadata": {},
   "source": [
    "Replace the input location depending on the smaple input video location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f195c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_1 = '/Users/srirupin/Downloads/target/cartwheel/Bodenturnen_2004_cartwheel_f_cm_np1_le_med_0.avi'\n",
    "sample_input_2 = '/Users/srirupin/Downloads/target/sword_exercise/Blade_Of_Fury_-_Scene_1_sword_exercise_f_cm_np1_ri_med_3.avi'\n",
    "sample_input_3 = '/Users/srirupin/Downloads/target/sword/AHF_longsword_against_Rapier_and_Dagger_Fight_sword_f_cm_np2_ri_bad_0.avi'\n",
    "sample_input_4 = '/Users/srirupin/Downloads/target/drink/CastAway2_drink_u_cm_np1_le_goo_8.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581288fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05643364042043686, 29, 'swordexcercise_output.json')\n",
      "(0.05719864368438721, 7, 'cartwheel_output.json')\n",
      "(0.058262795209884644, 38, 'cartwheel_output.json')\n",
      "(0.07027793675661087, 63, 'sword_output.json')\n",
      "(0.08527994155883789, 142, 'drink_output.json')\n",
      "(0.1098511666059494, 20, 'wave_output.json')\n",
      "(0.11364439874887466, 109, 'swordexcercise_output.json')\n",
      "(0.11459260806441307, 86, 'drink_output.json')\n",
      "(0.12074108421802521, 106, 'cartwheel_output.json')\n",
      "(0.12651728093624115, 75, 'ridebikeoutput.json')\n",
      "The Distance Measure:  0.05643364042043686\n",
      "The Distance Measure:  0.05719864368438721\n",
      "The Distance Measure:  0.058262795209884644\n",
      "The Distance Measure:  0.07027793675661087\n",
      "The Distance Measure:  0.08527994155883789\n",
      "The Distance Measure:  0.1098511666059494\n",
      "The Distance Measure:  0.11364439874887466\n",
      "The Distance Measure:  0.11459260806441307\n",
      "The Distance Measure:  0.12074108421802521\n",
      "The Distance Measure:  0.12651728093624115\n"
     ]
    }
   ],
   "source": [
    "ans = nearest_search(sample_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5ef469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05217835307121277, 4, 'drink_output.json')\n",
      "(0.05590900778770447, 106, 'drink_output.json')\n",
      "(0.05643364042043686, 29, 'swordexcercise_output.json')\n",
      "(0.056576214730739594, 39, 'drink_output.json')\n",
      "(0.05719864368438721, 7, 'cartwheel_output.json')\n",
      "(0.058262795209884644, 38, 'cartwheel_output.json')\n",
      "(0.07015926390886307, 18, 'ridebikeoutput.json')\n",
      "(0.07027793675661087, 63, 'sword_output.json')\n",
      "(0.07097502425312996, 37, 'wave_output.json')\n",
      "(0.07237998396158218, 57, 'ridebikeoutput.json')\n",
      "The Distance Measure:  0.05217835307121277\n",
      "The Distance Measure:  0.05590900778770447\n",
      "The Distance Measure:  0.05643364042043686\n",
      "The Distance Measure:  0.056576214730739594\n",
      "The Distance Measure:  0.05719864368438721\n",
      "The Distance Measure:  0.058262795209884644\n",
      "The Distance Measure:  0.07015926390886307\n",
      "The Distance Measure:  0.07027793675661087\n",
      "The Distance Measure:  0.07097502425312996\n",
      "The Distance Measure:  0.07237998396158218\n"
     ]
    }
   ],
   "source": [
    "ans_1 = nearest_search(sample_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd090d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05217835307121277, 4, 'drink_output.json')\n",
      "(0.05590900778770447, 106, 'drink_output.json')\n",
      "(0.05643364042043686, 29, 'swordexcercise_output.json')\n",
      "(0.056576214730739594, 39, 'drink_output.json')\n",
      "(0.05719864368438721, 7, 'cartwheel_output.json')\n",
      "(0.058262795209884644, 38, 'cartwheel_output.json')\n",
      "(0.07015926390886307, 18, 'ridebikeoutput.json')\n",
      "(0.07027793675661087, 63, 'sword_output.json')\n",
      "(0.07097502425312996, 37, 'wave_output.json')\n",
      "(0.07237998396158218, 57, 'ridebikeoutput.json')\n",
      "The Distance Measure:  0.05217835307121277\n",
      "The Distance Measure:  0.05590900778770447\n",
      "The Distance Measure:  0.05643364042043686\n",
      "The Distance Measure:  0.056576214730739594\n",
      "The Distance Measure:  0.05719864368438721\n",
      "The Distance Measure:  0.058262795209884644\n",
      "The Distance Measure:  0.07015926390886307\n",
      "The Distance Measure:  0.07027793675661087\n",
      "The Distance Measure:  0.07097502425312996\n",
      "The Distance Measure:  0.07237998396158218\n"
     ]
    }
   ],
   "source": [
    "ans_2 = nearest_search(sample_input_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083fde7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05217835307121277, 4, 'drink_output.json')\n",
      "(0.05590900778770447, 106, 'drink_output.json')\n",
      "(0.05643364042043686, 29, 'swordexcercise_output.json')\n",
      "(0.056576214730739594, 39, 'drink_output.json')\n",
      "(0.05719864368438721, 7, 'cartwheel_output.json')\n",
      "(0.058262795209884644, 38, 'cartwheel_output.json')\n",
      "(0.07015926390886307, 18, 'ridebikeoutput.json')\n",
      "(0.07027793675661087, 63, 'sword_output.json')\n",
      "(0.07097502425312996, 37, 'wave_output.json')\n",
      "(0.07237998396158218, 57, 'ridebikeoutput.json')\n",
      "The Distance Measure:  0.05217835307121277\n",
      "The Distance Measure:  0.05590900778770447\n",
      "The Distance Measure:  0.05643364042043686\n",
      "The Distance Measure:  0.056576214730739594\n",
      "The Distance Measure:  0.05719864368438721\n",
      "The Distance Measure:  0.058262795209884644\n",
      "The Distance Measure:  0.07015926390886307\n",
      "The Distance Measure:  0.07027793675661087\n",
      "The Distance Measure:  0.07097502425312996\n",
      "The Distance Measure:  0.07237998396158218\n"
     ]
    }
   ],
   "source": [
    "ans_3 = nearest_search(sample_input_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b104196",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = {\n",
    "    sample_input_1: ans,\n",
    "    sample_input_2: ans_1,\n",
    "    sample_input_3: ans_2,\n",
    "    sample_input_4: ans_3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c00013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_task1.json\", \"w\") as file:\n",
    "    json.dump(final_result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9473138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
